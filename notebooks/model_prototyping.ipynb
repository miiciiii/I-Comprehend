{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\miiciii\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\miiciii\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\miiciii\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\miiciii\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\miiciii\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"This sequence already has </s>.\")\n",
    "\n",
    "# Append path for module imports\n",
    "scripts_path = os.path.abspath(os.path.join('..', 'scripts'))\n",
    "sys.path.append(scripts_path)\n",
    "\n",
    "\n",
    "# Standard library imports\n",
    "import random\n",
    "import string\n",
    "\n",
    "# Third-party imports\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import pke\n",
    "import torch\n",
    "import nltk\n",
    "from dateutil.parser import parse\n",
    "from nltk.corpus import stopwords, wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from textdistance import levenshtein\n",
    "from rapidfuzz.distance import Levenshtein as levenshtein\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('brown')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from typing import List, Dict\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "d:\\01SetupFiles\\AnacondaNavigator\\anacondanavigatorfiles\\envs\\Thesis\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "d:\\01SetupFiles\\AnacondaNavigator\\anacondanavigatorfiles\\envs\\Thesis\\Lib\\site-packages\\sentence_transformers\\models\\Dense.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(input_path, \"pytorch_model.bin\"), map_location=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "# Import custom models from model_manager\n",
    "from model_manager import (\n",
    "    get_answergeneration_model,\n",
    "    get_questiongeneration_model,\n",
    "    get_sense2vec_model,\n",
    "    get_sentence_transformer_model,\n",
    "    get_random_passage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "t5ag_model, t5ag_tokenizer = get_answergeneration_model()\n",
    "t5qg_model, t5qg_tokenizer = get_questiongeneration_model()\n",
    "s2v = get_sense2vec_model()\n",
    "sentence_transformer_model = get_sentence_transformer_model()\n",
    "random_passage = get_random_passage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, context):\n",
    "    \"\"\"Generate an answer for a given question and context.\"\"\"\n",
    "    input_text = f\"question: {question} context: {context}\"\n",
    "    input_ids = t5ag_tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = t5ag_model.generate(input_ids, max_length=512, num_return_sequences=1, max_new_tokens=200)\n",
    "\n",
    "    return t5ag_tokenizer.decode(output[0], skip_special_tokens=True).capitalize()\n",
    "\n",
    "def get_passage(passage):\n",
    "    \"\"\"Generate a random context from the dataset.\"\"\"\n",
    "    return passage.sample(n=1)['context'].values[0]\n",
    "\n",
    "def get_question(context, answer, model, tokenizer):\n",
    "    \"\"\"Generate a question for the given answer and context.\"\"\"\n",
    "    answer_span = context.replace(answer, f\"<hl>{answer}<hl>\", 1) + \"</s>\"\n",
    "    inputs = tokenizer(answer_span, return_tensors=\"pt\")\n",
    "    question = model.generate(input_ids=inputs.input_ids, max_length=50)[0]\n",
    "\n",
    "    return tokenizer.decode(question, skip_special_tokens=True)\n",
    "\n",
    "def get_nouns_multipartite(content):\n",
    "    \"\"\"Extract keywords from content using MultipartiteRank algorithm.\"\"\"\n",
    "    try:\n",
    "        extractor = pke.unsupervised.MultipartiteRank()\n",
    "        extractor.load_document(input=content, language='en')\n",
    "        pos_tags = {'PROPN', 'NOUN', 'ADJ', 'VERB', 'ADP', 'ADV', 'DET', 'CONJ', 'NUM', 'PRON', 'X'}\n",
    "\n",
    "        stoplist = list(string.punctuation) + ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']\n",
    "        stoplist += stopwords.words('english')\n",
    "\n",
    "        extractor.candidate_selection(pos=pos_tags)\n",
    "        extractor.candidate_weighting(alpha=1.1, threshold=0.75, method='average')\n",
    "        keyphrases = extractor.get_n_best(n=15)\n",
    "        \n",
    "        return [val[0] for val in keyphrases]\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting nouns: {e}\")\n",
    "        return []\n",
    "\n",
    "def get_keywords(passage):\n",
    "    \"\"\"Extract keywords using TF-IDF.\"\"\"\n",
    "    try:\n",
    "        vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        tfidf_matrix = vectorizer.fit_transform([passage])\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        tfidf_scores = tfidf_matrix.toarray().flatten()\n",
    "        word_scores = dict(zip(feature_names, tfidf_scores))\n",
    "        sorted_words = sorted(word_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        keywords = [word for word, score in sorted_words]\n",
    "        return keywords\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting keywords: {e}\")\n",
    "        return []\n",
    "\n",
    "def filter_same_sense_words(original, wordlist):\n",
    "    \"\"\"Filter words that have the same sense as the original word.\"\"\"\n",
    "    base_sense = original.split('|')[1]\n",
    "    return [word[0].split('|')[0].replace(\"_\", \" \").title().strip() for word in wordlist if word[0].split('|')[1] == base_sense]\n",
    "\n",
    "def get_max_similarity_score(wordlist, word):\n",
    "    \"\"\"Get the maximum similarity score between the word and a list of words.\"\"\"\n",
    "    return max(levenshtein.normalized_similarity(word.lower(), each.lower()) for each in wordlist)\n",
    "\n",
    "def sense2vec_get_words(word, s2v, topn, question):\n",
    "    \"\"\"Get similar words using Sense2Vec.\"\"\"\n",
    "    try:\n",
    "        sense = s2v.get_best_sense(word, senses=[\"NOUN\", \"PERSON\", \"PRODUCT\", \"LOC\", \"ORG\", \"EVENT\", \"NORP\", \"WORK_OF_ART\", \"FAC\", \"GPE\", \"NUM\", \"FACILITY\"])\n",
    "        \n",
    "        if sense is None:\n",
    "            print(f\"[DEBUG] No suitable sense found for word: '{word}'\")\n",
    "            return []\n",
    "\n",
    "        most_similar = s2v.most_similar(sense, n=topn)\n",
    "        output = filter_same_sense_words(sense, most_similar)\n",
    "        \n",
    "        threshold = 0.6\n",
    "        final = [word]\n",
    "        checklist = question.split()\n",
    "\n",
    "        for similar_word in output:\n",
    "            if get_max_similarity_score(final, similar_word) < threshold and similar_word not in final and similar_word not in checklist:\n",
    "                final.append(similar_word)\n",
    "    \n",
    "        return final[1:]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in Sense2Vec: {e}\")\n",
    "        return []\n",
    "\n",
    "def mmr(doc_embedding, word_embeddings, words, top_n, lambda_param):\n",
    "    \"\"\"Maximal Marginal Relevance (MMR) for keyword extraction.\"\"\"\n",
    "    try:\n",
    "        word_doc_similarity = cosine_similarity(word_embeddings, doc_embedding)\n",
    "        word_similarity = cosine_similarity(word_embeddings)\n",
    "\n",
    "        keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "        candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "        for _ in range(top_n - 1):\n",
    "            candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "            target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "\n",
    "            mmr = (lambda_param * candidate_similarities) - ((1 - lambda_param) * target_similarities.reshape(-1, 1))\n",
    "            mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "\n",
    "            keywords_idx.append(mmr_idx)\n",
    "            candidates_idx.remove(mmr_idx)\n",
    "\n",
    "        return [words[idx] for idx in keywords_idx]\n",
    "    except Exception as e:\n",
    "        print(f\"Error in MMR: {e}\")\n",
    "        return []\n",
    "    \n",
    "def classify_question_type(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Classify the type of question as literal, evaluative, or inferential.\n",
    "    \n",
    "    Parameters:\n",
    "        question (str): The question to classify.\n",
    "        \n",
    "    Returns:\n",
    "        str: The type of the question ('literal', 'evaluative', or 'inferential').\n",
    "    \"\"\"\n",
    "    # Define keywords or patterns for each question type\n",
    "    literal_keywords = [\n",
    "    'what', 'when', 'where', 'who', 'how many', 'how much', \n",
    "    'which', 'name', 'list', 'identify', 'define', 'describe', \n",
    "    'state', 'mention'\n",
    "    ]\n",
    "\n",
    "    evaluative_keywords = [\n",
    "    'evaluate', 'justify', 'explain why', 'assess', 'critique', \n",
    "    'discuss', 'judge', 'opinion', 'argue', 'agree or disagree', \n",
    "    'defend', 'support your answer', 'weigh the pros and cons', \n",
    "    'compare', 'contrast'\n",
    "    ]\n",
    "\n",
    "    inferential_keywords = [\n",
    "    'why', 'how', 'what if', 'predict', 'suggest', 'imply', \n",
    "    'conclude', 'infer', 'reason', 'what might', 'what could', \n",
    "    'what would happen if', 'speculate', 'deduce', 'interpret', \n",
    "    'hypothesize', 'assume'\n",
    "    ]\n",
    "\n",
    "\n",
    "    question_lower = question.lower()\n",
    "    \n",
    "    # Check for literal question keywords\n",
    "    if any(keyword in question_lower for keyword in literal_keywords):\n",
    "        return 'literal'\n",
    "    \n",
    "    # Check for evaluative question keywords\n",
    "    if any(keyword in question_lower for keyword in evaluative_keywords):\n",
    "        return 'evaluative'\n",
    "    \n",
    "    # Check for inferential question keywords\n",
    "    if any(keyword in question_lower for keyword in inferential_keywords):\n",
    "        return 'inferential'\n",
    "    \n",
    "    # Default to 'unknown' if no pattern matches\n",
    "    return 'unknown'\n",
    "\n",
    "def get_distractors_wordnet(word):\n",
    "    \"\"\"Get distractors using WordNet.\"\"\"\n",
    "    distractors = []\n",
    "    try:\n",
    "        synsets = wn.synsets(word, pos='n')\n",
    "        if not synsets:\n",
    "            print(f\"[DEBUG] No synsets found for word: '{word}'\")\n",
    "            return distractors\n",
    "\n",
    "        synset = synsets[0]\n",
    "        hypernyms = synset.hypernyms()\n",
    "        if not hypernyms:\n",
    "            print(f\"[DEBUG] No hypernyms found for synset of word: '{word}'\")\n",
    "            return distractors\n",
    "\n",
    "        hypernym = hypernyms[0]\n",
    "        hyponyms = hypernym.hyponyms()\n",
    "        if not hyponyms:\n",
    "            print(f\"[DEBUG] No hyponyms found for hypernym of word: '{word}'\")\n",
    "            return distractors\n",
    "\n",
    "        for item in hyponyms:\n",
    "            name = item.lemmas()[0].name().replace(\"_\", \" \").title()\n",
    "            if name.lower() != word.lower() and name not in distractors:\n",
    "                distractors.append(name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in WordNet distractors: {e}\")\n",
    "    return distractors\n",
    "\n",
    "def get_distractors(phrase, original_sentence, sense2vec_model, sentence_model, top_n, lambda_val):\n",
    "    \"\"\"Get distractors for a given phrase using various methods.\"\"\"\n",
    "    try:\n",
    "        if any(char.isdigit() for char in phrase):\n",
    "            # Handle as a date or numerical distractor\n",
    "            if is_date(phrase):\n",
    "                return generate_date_distractors(phrase)\n",
    "            else:\n",
    "                return generate_numerical_distractors(phrase)\n",
    "        \n",
    "        words = phrase.split()\n",
    "        distractors = set()\n",
    "\n",
    "        if len(words) == 1:\n",
    "            word = words[0]\n",
    "            distractors.update(get_distractors_for_single_word(word, original_sentence, sense2vec_model, top_n))\n",
    "        else:\n",
    "            word_distractors = []\n",
    "            for word in words:\n",
    "                word_distractors.append(list(get_distractors_for_single_word(word, original_sentence, sense2vec_model, top_n)))\n",
    "            \n",
    "            distractor_combinations = set()\n",
    "            for combination in itertools.product(*word_distractors):\n",
    "                combined_distractor = ' '.join(combination)\n",
    "                if combined_distractor.lower() != phrase.lower() and len(combination) == len(words):\n",
    "                    distractor_combinations.add(combined_distractor.capitalize())\n",
    "            distractors.update(distractor_combinations)\n",
    "\n",
    "        distractors.add(phrase.capitalize())\n",
    "        embedding_sentence = f\"{original_sentence} {phrase.capitalize()}\"\n",
    "        keyword_embedding = sentence_model.encode([embedding_sentence])\n",
    "        distractor_embeddings = sentence_model.encode(list(distractors))\n",
    "        max_keywords = min(len(distractors), 5)\n",
    "        filtered_keywords = mmr(keyword_embedding, distractor_embeddings, list(distractors), max_keywords, lambda_val)\n",
    "        final_distractors = [kw.capitalize() for kw in filtered_keywords if kw.lower() != phrase.lower()]\n",
    "        return final_distractors\n",
    "    except Exception as e:\n",
    "        print(f\"Error in getting distractors: {e}\")\n",
    "        return []\n",
    "\n",
    "def get_distractors_for_single_word(word, original_sentence, sense2vec_model, top_n):\n",
    "    \"\"\"Helper function to get distractors for a single word.\"\"\"\n",
    "    distractors = set()\n",
    "    try:\n",
    "        distractors_word = sense2vec_get_words(word, sense2vec_model, top_n, original_sentence)\n",
    "        if distractors_word:\n",
    "            distractors.update(distractors_word)\n",
    "        \n",
    "        if not distractors:\n",
    "            distractors_wordnet = get_distractors_wordnet(word)\n",
    "            if distractors_wordnet:\n",
    "                distractors.update(distractors_wordnet)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in single word distractors: {e}\")\n",
    "    return distractors\n",
    "\n",
    "def is_date(string):\n",
    "    \"\"\"Check if a string is a date.\"\"\"\n",
    "    try:\n",
    "        parse(string, fuzzy=False)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def generate_date_distractors(date_string):\n",
    "    \"\"\"Generate distractors for date phrases.\"\"\"\n",
    "    try:\n",
    "        date = parse(date_string)\n",
    "        distractors = set()\n",
    "        for _ in range(5):\n",
    "            year = date.year + random.choice([-5, -3, -1, 1, 3, 5])\n",
    "            month = random.randint(1, 12)\n",
    "            day = min(random.randint(1, 28), 28)\n",
    "            new_date = date.replace(year=year, month=month, day=day)\n",
    "            distractors.add(new_date.strftime(\"%m/%d/%Y\"))\n",
    "        return list(distractors)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating date distractors: {e}\")\n",
    "        return []\n",
    "\n",
    "def generate_numerical_distractors(number_string):\n",
    "    \"\"\"Generate distractors for numerical phrases.\"\"\"\n",
    "    try:\n",
    "        # Parse the number from the string, handling integer and float types\n",
    "        number = float(number_string) if '.' in number_string else int(number_string)\n",
    "        distractors = set()\n",
    "\n",
    "        # Generate random variations around the original number\n",
    "        for _ in range(5):\n",
    "            variation = random.uniform(-0.3, 0.3) * number\n",
    "            new_number = number + variation\n",
    "            distractors.add(round(new_number, 2))\n",
    "\n",
    "        # Ensure the original number is not in the distractors\n",
    "        distractors.discard(number)\n",
    "        return list(distractors)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating numerical distractors: {e}\")\n",
    "        return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mca_questions(context, qg_model, qg_tokenizer, s2v, sentence_transformer_model, num_questions=5, max_attempts=2) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Generate multiple-choice questions for a given context.\n",
    "    \n",
    "    Parameters:\n",
    "        context (str): The context from which questions are generated.\n",
    "        qg_model (T5ForConditionalGeneration): The question generation model.\n",
    "        qg_tokenizer (T5Tokenizer): The tokenizer for the question generation model.\n",
    "        s2v (Sense2Vec): The Sense2Vec model for finding similar words.\n",
    "        sentence_transformer_model (SentenceTransformer): The sentence transformer model for embeddings.\n",
    "        num_questions (int): The number of questions to generate.\n",
    "        max_attempts (int): The maximum number of attempts to generate questions.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of dictionaries with questions and their corresponding distractors.\n",
    "    \"\"\"\n",
    "    output_list = []\n",
    "\n",
    "    imp_keywords = get_keywords(context)  # Extract keywords only once\n",
    "    print(f\"[DEBUG] Length: {len(imp_keywords)}, Extracted keywords: {imp_keywords}\")\n",
    "\n",
    "    generated_questions = set()\n",
    "    generated_answers = set()  # Track generated answers\n",
    "    attempts = 0\n",
    "\n",
    "    while len(output_list) < num_questions and attempts < max_attempts:\n",
    "        attempts += 1\n",
    "\n",
    "        for keyword in imp_keywords:\n",
    "            if len(output_list) >= num_questions:\n",
    "                break\n",
    "            \n",
    "            question = get_question(context, keyword, qg_model, qg_tokenizer)\n",
    "            print(f\"[DEBUG] Generated question: '{question}' for keyword: '{keyword}'\")\n",
    "            \n",
    "            # Encode the new question\n",
    "            new_question_embedding = sentence_transformer_model.encode(question, convert_to_tensor=True)\n",
    "            is_similar = False\n",
    "\n",
    "            # Check similarity with existing questions\n",
    "            for generated_q in generated_questions:\n",
    "                existing_question_embedding = sentence_transformer_model.encode(generated_q, convert_to_tensor=True)\n",
    "                similarity = cosine_similarity(new_question_embedding.unsqueeze(0), existing_question_embedding.unsqueeze(0))[0][0]\n",
    "\n",
    "                if similarity > 0.8:\n",
    "                    is_similar = True\n",
    "                    print(f\"[DEBUG] Question '{question}' is too similar to an existing question, skipping.\")\n",
    "                    break\n",
    "\n",
    "            if is_similar:\n",
    "                continue\n",
    "\n",
    "            # Generate and check answer\n",
    "            t5_answer = answer_question(question, context)\n",
    "            print(f\"[DEBUG] Generated answer: '{t5_answer}' for question: '{question}'\")\n",
    "            \n",
    "            # Skip answers longer than 3 words\n",
    "            if len(t5_answer.split()) > 3:\n",
    "                print(f\"[DEBUG] Answer '{t5_answer}' is too long, skipping.\")\n",
    "                continue\n",
    "\n",
    "            if t5_answer in generated_answers:\n",
    "                print(f\"[DEBUG] Answer '{t5_answer}' has already been generated, skipping question.\")\n",
    "                continue\n",
    "\n",
    "            generated_questions.add(question)\n",
    "            generated_answers.add(t5_answer)\n",
    "\n",
    "            # Generate distractors\n",
    "            distractors = get_distractors(t5_answer, question, s2v, sentence_transformer_model, 40, 0.2)\n",
    "            if len(distractors) == 0:\n",
    "                print(\"[DEBUG] No distractors found, using important keywords as distractors.\")\n",
    "                distractors = imp_keywords\n",
    "\n",
    "            # If answer is a phrase, handle distractors appropriately\n",
    "            answer_words = t5_answer.split()\n",
    "            distractors = [d for d in distractors if not any(word.lower() in t5_answer.lower() for word in answer_words)]\n",
    "            distractors = [d.capitalize() for d in distractors if d.lower() != keyword.lower()]\n",
    "\n",
    "            if len(distractors) < 3:\n",
    "                additional_distractors = [kw.capitalize() for kw in imp_keywords if kw.capitalize() not in distractors and kw.lower() != keyword.lower()]\n",
    "                distractors.extend(additional_distractors[:3 - len(distractors)])\n",
    "            else:\n",
    "                distractors = distractors[:3]\n",
    "\n",
    "            print(f\"[DEBUG] Final distractors: {distractors} for question: '{question}'\")\n",
    "\n",
    "            choices = distractors + [t5_answer]\n",
    "            choices = [item.title() for item in choices]\n",
    "            random.shuffle(choices)\n",
    "            print(f\"[DEBUG] Options: {choices} for question: '{question}'\")\n",
    "\n",
    "            # Classify question type\n",
    "            question_type = classify_question_type(question)\n",
    "            \n",
    "            output_list.append({\n",
    "                'answer': t5_answer,\n",
    "                'answer_length': len(t5_answer),\n",
    "                'choices': choices,\n",
    "                'passage': context,\n",
    "                'passage_length': len(context),\n",
    "                'question': question,\n",
    "                'question_length': len(question),\n",
    "                'question_type': question_type\n",
    "            })\n",
    "\n",
    "        print(f\"[DEBUG] Generated {len(output_list)} questions so far after {attempts} attempts\")\n",
    "\n",
    "    return output_list[:num_questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On 12 January 1953, he was appointed Patriarch of Venice and, accordingly, raised to the rank of Cardinal-Priest of Santa Prisca by Pope Pius XII. Roncalli left France for Venice on 23 February 1953 stopping briefly in Milan and then to Rome. On 15 March 1953, he took possession of his new diocese in Venice. As a sign of his esteem, the President of France, Vincent Auriol, claimed the ancient privilege possessed by French monarchs and bestowed the red biretta on Roncalli at a ceremony in the Élysée Palace. It was around this time that he, with the aid of Monsignor Bruno Heim, formed his coat of arms with a lion of Saint Mark on a white ground. Auriol also awarded Roncalli three months later with the award of Commander of the Legion of Honour.\n",
      "[DEBUG] Length: 68, Extracted keywords: ['1953', 'roncalli', 'venice', 'auriol', 'france', '12', '15', '23', 'accordingly', 'aid', 'ancient', 'appointed', 'arms', 'award', 'awarded', 'bestowed', 'biretta', 'briefly', 'bruno', 'cardinal', 'ceremony', 'claimed', 'coat', 'commander', 'diocese', 'esteem', 'february', 'formed', 'french', 'ground', 'heim', 'honour', 'january', 'later', 'left', 'legion', 'lion', 'march', 'mark', 'milan', 'monarchs', 'monsignor', 'months', 'new', 'palace', 'patriarch', 'pius', 'pope', 'possessed', 'possession', 'president', 'priest', 'prisca', 'privilege', 'raised', 'rank', 'red', 'rome', 'saint', 'santa', 'sign', 'stopping', 'time', 'took', 'vincent', 'white', 'xii', 'élysée']\n",
      "[DEBUG] Generated question: 'In what year was he appointed Patriarch of Venice?' for keyword: '1953'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=200) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Generated answer: '1953' for question: 'In what year was he appointed Patriarch of Venice?'\n",
      "[DEBUG] Final distractors: ['Roncalli', 'Venice', 'Auriol'] for question: 'In what year was he appointed Patriarch of Venice?'\n",
      "[DEBUG] Options: ['Auriol', 'Venice', 'Roncalli', '1953'] for question: 'In what year was he appointed Patriarch of Venice?'\n",
      "[DEBUG] Generated question: 'What was the name of the diocese in which he was diocese?' for keyword: 'roncalli'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=200) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Generated answer: 'Venice' for question: 'What was the name of the diocese in which he was diocese?'\n",
      "[DEBUG] Final distractors: ['1953', 'Venice', 'Auriol'] for question: 'What was the name of the diocese in which he was diocese?'\n",
      "[DEBUG] Options: ['Venice', 'Venice', 'Auriol', '1953'] for question: 'What was the name of the diocese in which he was diocese?'\n",
      "[DEBUG] Generated question: 'What was the name of the diocese in which he was diocese?' for keyword: 'venice'\n",
      "[DEBUG] Question 'What was the name of the diocese in which he was diocese?' is too similar to an existing question, skipping.\n",
      "[DEBUG] Generated question: 'What was the name of the diocese in which he was diocese?' for keyword: 'auriol'\n",
      "[DEBUG] Question 'What was the name of the diocese in which he was diocese?' is too similar to an existing question, skipping.\n",
      "[DEBUG] Generated question: 'What was the name of the diocese in which he was diocese?' for keyword: 'france'\n",
      "[DEBUG] Question 'What was the name of the diocese in which he was diocese?' is too similar to an existing question, skipping.\n",
      "[DEBUG] Generated question: 'When was he appointed Patriarch of Venice?' for keyword: '12'\n",
      "[DEBUG] Question 'When was he appointed Patriarch of Venice?' is too similar to an existing question, skipping.\n",
      "[DEBUG] Generated question: 'When did he take possession of his new diocese in Venice?' for keyword: '15'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=200) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Generated answer: '15 march 1953' for question: 'When did he take possession of his new diocese in Venice?'\n",
      "[DEBUG] Final distractors: ['1953', 'Roncalli', 'Venice'] for question: 'When did he take possession of his new diocese in Venice?'\n",
      "[DEBUG] Options: ['15 March 1953', 'Roncalli', '1953', 'Venice'] for question: 'When did he take possession of his new diocese in Venice?'\n",
      "[DEBUG] Generated question: 'When did Roncalli leave France for Venice?' for keyword: '23'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=200) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Generated answer: '1953' for question: 'When did Roncalli leave France for Venice?'\n",
      "[DEBUG] Answer '1953' has already been generated, skipping question.\n",
      "[DEBUG] Generated question: 'How did he become Patriarch of Venice?' for keyword: 'accordingly'\n",
      "[DEBUG] Question 'How did he become Patriarch of Venice?' is too similar to an existing question, skipping.\n",
      "[DEBUG] Generated question: 'What did Monsignor Bruno Heim provide to Roncalli?' for keyword: 'aid'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=200) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Generated answer: 'Vincent auriol' for question: 'What did Monsignor Bruno Heim provide to Roncalli?'\n",
      "[DEBUG] No suitable sense found for word: 'auriol'\n",
      "[DEBUG] No synsets found for word: 'auriol'\n",
      "[DEBUG] No distractors found, using important keywords as distractors.\n",
      "[DEBUG] Final distractors: ['1953', 'Roncalli', 'Venice'] for question: 'What did Monsignor Bruno Heim provide to Roncalli?'\n",
      "[DEBUG] Options: ['Vincent Auriol', '1953', 'Venice', 'Roncalli'] for question: 'What did Monsignor Bruno Heim provide to Roncalli?'\n",
      "[DEBUG] Generated question: 'What privilege did Auriol claim that French monarchs had?' for keyword: 'ancient'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=200) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Generated answer: 'Ancient privilege' for question: 'What privilege did Auriol claim that French monarchs had?'\n",
      "[DEBUG] Final distractors: ['1953', 'Roncalli', 'Venice'] for question: 'What privilege did Auriol claim that French monarchs had?'\n",
      "[DEBUG] Options: ['Venice', 'Ancient Privilege', 'Roncalli', '1953'] for question: 'What privilege did Auriol claim that French monarchs had?'\n",
      "[DEBUG] Generated 5 questions so far after 1 attempts\n",
      "Data successfully written to ..\\outputs\\plots\\predictions\\generated_questions.json.\n"
     ]
    }
   ],
   "source": [
    "original_context = get_passage(random_passage)\n",
    "\n",
    "print(original_context)\n",
    "\n",
    "questions_and_distractors = get_mca_questions(original_context, t5qg_model, t5qg_tokenizer, s2v, sentence_transformer_model, num_questions=5)\n",
    "\n",
    "OUTPUT_FILE = os.path.join('..', 'outputs', 'plots', 'predictions', 'generated_questions.json')\n",
    "\n",
    "def convert_to_serializable(data):\n",
    "    if isinstance(data, pd.Series):\n",
    "        return data.tolist()\n",
    "    elif isinstance(data, dict):\n",
    "        return {k: convert_to_serializable(v) for k, v in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [convert_to_serializable(i) for i in data]\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "try:\n",
    "    with open(OUTPUT_FILE, 'r') as f:\n",
    "        existing_data = json.load(f)\n",
    "        if not isinstance(existing_data, list):  # Ensure existing data is a list\n",
    "            raise ValueError(\"The existing data is not a list.\")\n",
    "except FileNotFoundError:\n",
    "    existing_data = []\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Warning: {OUTPUT_FILE} contains invalid JSON. Initializing with an empty list.\")\n",
    "    existing_data = []\n",
    "except ValueError as e:\n",
    "    print(f\"ValueError: {e}\")\n",
    "    existing_data = []\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred while reading {OUTPUT_FILE}: {e}\")\n",
    "    existing_data = []\n",
    "\n",
    "# Convert any non-serializable data to a serializable format\n",
    "questions_and_distractors_serializable = convert_to_serializable(questions_and_distractors)\n",
    "existing_data.extend(questions_and_distractors_serializable)\n",
    "\n",
    "# Write updated data back to the file\n",
    "try:\n",
    "    with open(OUTPUT_FILE, 'w') as f:\n",
    "        json.dump(existing_data, f, indent=4)\n",
    "    print(f\"Data successfully written to {OUTPUT_FILE}.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while writing to {OUTPUT_FILE}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
