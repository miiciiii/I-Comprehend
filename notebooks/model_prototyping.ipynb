{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\miiciii\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\miiciii\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\miiciii\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\miiciii\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\miiciii\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"This sequence already has </s>.\")\n",
    "\n",
    "# Append path for module imports\n",
    "scripts_path = os.path.abspath(os.path.join('..', 'scripts'))\n",
    "sys.path.append(scripts_path)\n",
    "\n",
    "\n",
    "# Standard library imports\n",
    "import random\n",
    "import string\n",
    "\n",
    "# Third-party imports\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import pke\n",
    "import torch\n",
    "import nltk\n",
    "from dateutil.parser import parse\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords, wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from textdistance import levenshtein\n",
    "from rapidfuzz import fuzz\n",
    "from rapidfuzz.distance import Levenshtein as levenshtein\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('brown')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from typing import List, Dict\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "d:\\01SetupFiles\\AnacondaNavigator\\anacondanavigatorfiles\\envs\\Thesis\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "d:\\01SetupFiles\\AnacondaNavigator\\anacondanavigatorfiles\\envs\\Thesis\\Lib\\site-packages\\sentence_transformers\\models\\Dense.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(input_path, \"pytorch_model.bin\"), map_location=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "# Import custom models from model_manager\n",
    "from model_manager import (\n",
    "    get_answergeneration_model,\n",
    "    get_questiongeneration_model,\n",
    "    get_sense2vec_model,\n",
    "    get_sentence_transformer_model,\n",
    "    get_random_passage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "t5ag_model, t5ag_tokenizer = get_answergeneration_model()\n",
    "t5qg_model, t5qg_tokenizer = get_questiongeneration_model()\n",
    "s2v = get_sense2vec_model()\n",
    "sentence_transformer_model = get_sentence_transformer_model()\n",
    "random_passage = get_random_passage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, context):\n",
    "    \"\"\"Generate an answer for a given question and context.\"\"\"\n",
    "    input_text = f\"question: {question} context: {context}\"\n",
    "    input_ids = t5ag_tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = t5ag_model.generate(input_ids, max_length=512, num_return_sequences=1, max_new_tokens=200)\n",
    "\n",
    "    return t5ag_tokenizer.decode(output[0], skip_special_tokens=True).capitalize()\n",
    "\n",
    "def get_passage(passage):\n",
    "    \"\"\"Generate a random context from the dataset.\"\"\"\n",
    "    return passage.sample(n=1)['context'].values[0]\n",
    "\n",
    "def get_question(context, answer, model, tokenizer):\n",
    "    \"\"\"Generate a question for the given answer and context.\"\"\"\n",
    "    answer_span = context.replace(answer, f\"<hl>{answer}<hl>\", 1) + \"</s>\"\n",
    "    inputs = tokenizer(answer_span, return_tensors=\"pt\")\n",
    "    question = model.generate(input_ids=inputs.input_ids, max_length=50)[0]\n",
    "\n",
    "    return tokenizer.decode(question, skip_special_tokens=True)\n",
    "\n",
    "def get_keywords(passage):\n",
    "    \"\"\"Extract keywords using TF-IDF.\"\"\"\n",
    "    try:\n",
    "        vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        tfidf_matrix = vectorizer.fit_transform([passage])\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        tfidf_scores = tfidf_matrix.toarray().flatten() # type: ignore\n",
    "        word_scores = dict(zip(feature_names, tfidf_scores))\n",
    "        sorted_words = sorted(word_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        keywords = [word for word, score in sorted_words]\n",
    "        return keywords\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting keywords: {e}\")\n",
    "        return []\n",
    "\n",
    "def classify_question_type(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Classify the type of question as literal, evaluative, or inferential.\n",
    "    \n",
    "    Parameters:\n",
    "        question (str): The question to classify.\n",
    "        \n",
    "    Returns:\n",
    "        str: The type of the question ('literal', 'evaluative', or 'inferential').\n",
    "    \"\"\"\n",
    "    # Define keywords or patterns for each question type\n",
    "    literal_keywords = [\n",
    "    'what', 'when', 'where', 'who', 'how many', 'how much', \n",
    "    'which', 'name', 'list', 'identify', 'define', 'describe', \n",
    "    'state', 'mention'\n",
    "    ]\n",
    "\n",
    "    evaluative_keywords = [\n",
    "    'evaluate', 'justify', 'explain why', 'assess', 'critique', \n",
    "    'discuss', 'judge', 'opinion', 'argue', 'agree or disagree', \n",
    "    'defend', 'support your answer', 'weigh the pros and cons', \n",
    "    'compare', 'contrast'\n",
    "    ]\n",
    "\n",
    "    inferential_keywords = [\n",
    "    'why', 'how', 'what if', 'predict', 'suggest', 'imply', \n",
    "    'conclude', 'infer', 'reason', 'what might', 'what could', \n",
    "    'what would happen if', 'speculate', 'deduce', 'interpret', \n",
    "    'hypothesize', 'assume'\n",
    "    ]\n",
    "\n",
    "\n",
    "    question_lower = question.lower()\n",
    "    \n",
    "    # Check for literal question keywords\n",
    "    if any(keyword in question_lower for keyword in literal_keywords):\n",
    "        return 'literal'\n",
    "    \n",
    "    # Check for evaluative question keywords\n",
    "    if any(keyword in question_lower for keyword in evaluative_keywords):\n",
    "        return 'evaluative'\n",
    "    \n",
    "    # Check for inferential question keywords\n",
    "    if any(keyword in question_lower for keyword in inferential_keywords):\n",
    "        return 'inferential'\n",
    "    \n",
    "    # Default to 'unknown' if no pattern matches\n",
    "    return 'unknown'\n",
    "\n",
    "def format_phrase(phrase):\n",
    "    \"\"\"Format phrases by replacing spaces with underscores and adding default |n.\"\"\"\n",
    "    return phrase.replace(\" \", \"_\") + \"|n\"\n",
    "\n",
    "def is_valid_distractor(distractor, input_phrase):\n",
    "    \"\"\"Check if the distractor is valid by ensuring it's alphabetic and relevant.\"\"\"\n",
    "    if not re.match(r'^[a-zA-Z\\s]+$', distractor):  # Only allow alphabetic characters and spaces\n",
    "        return False\n",
    "    \n",
    "    word_count = len(distractor.split())\n",
    "    if word_count < 1 or word_count > 4:  # Only allow distractors with 1-4 words\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def get_max_similarity_score(wordlist, word):\n",
    "    \"\"\"Get the maximum similarity score between the word and a list of words.\"\"\"\n",
    "    return max(fuzz.ratio(word.lower(), each.lower()) / 100 for each in wordlist)\n",
    "\n",
    "def filter_same_sense_words(original, wordlist):\n",
    "    \"\"\"Filter words that have the same sense as the original word.\"\"\"\n",
    "    base_sense = original.split('|')[1]\n",
    "    return [word[0].split('|')[0].replace(\"_\", \" \").title().strip() for word in wordlist if word[0].split('|')[1] == base_sense]\n",
    "\n",
    "def sense2vec_get_words(word, s2v, topn, question):\n",
    "    \"\"\"Get similar words using Sense2Vec and filter for sense and similarity.\"\"\"\n",
    "    try:\n",
    "        sense = s2v.get_best_sense(word, senses=[\"NOUN\", \"PERSON\", \"PRODUCT\", \"LOC\", \"ORG\", \"EVENT\", \"NORP\", \"WORK_OF_ART\", \"FAC\", \"GPE\", \"NUM\", \"FACILITY\"])\n",
    "        \n",
    "        if sense is None:\n",
    "            print(f\"[DEBUG] No suitable sense found for word: '{word}'\")\n",
    "            return []\n",
    "\n",
    "        most_similar = s2v.most_similar(sense, n=topn * 2)\n",
    "        output = filter_same_sense_words(sense, most_similar)\n",
    "        \n",
    "        threshold = 0.6\n",
    "        final = [word]\n",
    "        checklist = question.split()\n",
    "\n",
    "        for similar_word in output:\n",
    "            if get_max_similarity_score(final, similar_word) < threshold and similar_word not in final and similar_word not in checklist:\n",
    "                final.append(similar_word)\n",
    "    \n",
    "        return final[1:topn+1]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in Sense2Vec: {e}\")\n",
    "        return []\n",
    "\n",
    "def calculate_similarity(phrases):\n",
    "    \"\"\"Calculate semantic similarity between the input phrase and possible distractors.\"\"\"\n",
    "    if len(phrases) < 2:  # At least two phrases needed to calculate similarity\n",
    "        return []\n",
    "    \n",
    "    vectorizer = TfidfVectorizer().fit_transform(phrases)\n",
    "    vectors = vectorizer.toarray()\n",
    "    \n",
    "    if vectors.shape[0] < 2:  # Need at least two vectors for comparison\n",
    "        print(f\"[DEBUG] Insufficient vectors for similarity calculation. Vectors shape: {vectors.shape}\")\n",
    "        return []\n",
    "    \n",
    "    cosine_similarities = cosine_similarity([vectors[0]], vectors[1:])\n",
    "    return cosine_similarities[0]\n",
    "\n",
    "def filter_distractors(input_phrase, similar_keywords, topn):\n",
    "    \"\"\"Filter distractors to match word count, avoid identical or stem-similar words, and check format.\"\"\"\n",
    "    word_count = len(input_phrase.split())\n",
    "    filtered_keywords = []\n",
    "    stemmer = PorterStemmer()\n",
    "    input_stem = stemmer.stem(input_phrase.lower())\n",
    "\n",
    "    valid_keywords = [\n",
    "        keyword for keyword in similar_keywords\n",
    "        if len(keyword.split()) == word_count and \n",
    "        keyword.lower() != input_phrase.lower() and\n",
    "        stemmer.stem(keyword.lower()) != input_stem and\n",
    "        is_valid_distractor(keyword, input_phrase)\n",
    "    ]\n",
    "    \n",
    "    if len(valid_keywords) == 0:\n",
    "        print(f\"[DEBUG] No valid distractors found for '{input_phrase}'.\")\n",
    "        return []\n",
    "\n",
    "    phrases_to_compare = [input_phrase] + valid_keywords\n",
    "    similarities = calculate_similarity(phrases_to_compare)\n",
    "\n",
    "    if not similarities:\n",
    "        print(f\"[DEBUG] No similarities calculated.\")\n",
    "        return []\n",
    "\n",
    "    for idx, similarity in enumerate(similarities):\n",
    "        if 0.3 <= similarity < 0.8:  # Distractors should be relevant but not too close\n",
    "            filtered_keywords.append(valid_keywords[idx])\n",
    "        \n",
    "        if len(filtered_keywords) == topn:\n",
    "            break\n",
    "\n",
    "    return filtered_keywords\n",
    "\n",
    "def mmr(doc_embedding, word_embeddings, words, top_n, lambda_param):\n",
    "    \"\"\"Maximal Marginal Relevance (MMR) for keyword extraction.\"\"\"\n",
    "    try:\n",
    "        word_doc_similarity = cosine_similarity(word_embeddings, doc_embedding)\n",
    "        word_similarity = cosine_similarity(word_embeddings)\n",
    "\n",
    "        keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "        candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "        for _ in range(top_n - 1):\n",
    "            candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "            target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "\n",
    "            mmr = (lambda_param * candidate_similarities) - ((1 - lambda_param) * target_similarities.reshape(-1, 1))\n",
    "            mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "\n",
    "            keywords_idx.append(mmr_idx)\n",
    "            candidates_idx.remove(mmr_idx)\n",
    "\n",
    "        return [words[idx] for idx in keywords_idx]\n",
    "    except Exception as e:\n",
    "        print(f\"Error in MMR: {e}\")\n",
    "        return []\n",
    "\n",
    "def get_nouns_multipartite(content):\n",
    "    \"\"\"Extract keywords from content using MultipartiteRank algorithm.\"\"\"\n",
    "    try:\n",
    "        extractor = pke.unsupervised.MultipartiteRank()  # type: ignore\n",
    "        extractor.load_document(input=content, language='en')\n",
    "        pos_tags = {'PROPN', 'NOUN', 'ADJ', 'VERB', 'ADP', 'ADV', 'DET', 'CONJ', 'NUM', 'PRON', 'X'}\n",
    "\n",
    "        stoplist = list(string.punctuation) + ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']\n",
    "        stoplist += stopwords.words('english')\n",
    "\n",
    "        extractor.candidate_selection(pos=pos_tags)\n",
    "        extractor.candidate_weighting(alpha=1.1, threshold=0.75, method='average')\n",
    "        keyphrases = extractor.get_n_best(n=15)\n",
    "\n",
    "        return [val[0] for val in keyphrases]\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting nouns: {e}\")\n",
    "        return []\n",
    "\n",
    "def get_distractors(input_phrases, content, topn=5):\n",
    "    \"\"\"Generate distractors for input phrases using Sense2Vec, MultipartiteRank, and filtering logic.\"\"\"\n",
    "    result_list = []\n",
    "\n",
    "    # Extract key phrases using MultipartiteRank from the content\n",
    "    imp_keywords = get_nouns_multipartite(content)\n",
    "\n",
    "    for phrase in input_phrases:\n",
    "        formatted_phrase = format_phrase(phrase)\n",
    "\n",
    "        # Try to get similar words using Sense2Vec\n",
    "        similar_keywords = sense2vec_get_words(phrase, s2v, topn * 2, phrase)\n",
    "        \n",
    "        # If Sense2Vec doesn't return enough results, fall back to keywords from MultipartiteRank\n",
    "        if len(similar_keywords) < topn:\n",
    "            print(f\"[DEBUG] Fallback to Multipartite keywords for '{phrase}' due to insufficient distractors.\")\n",
    "            similar_keywords += imp_keywords\n",
    "\n",
    "        # Filter distractors to match word count, format, and semantic similarity\n",
    "        final_distractors = filter_distractors(phrase, similar_keywords, topn)\n",
    "        \n",
    "        result_list.append({\n",
    "            \"phrase\": phrase,\n",
    "            \"similar_keywords\": final_distractors\n",
    "        })\n",
    "\n",
    "    return result_list\n",
    "\n",
    "\n",
    "def extract_similar_keywords(input_phrases, topn=5):\n",
    "    \"\"\"Call get_distractors and extract only the similar_keywords values.\"\"\"\n",
    "    try:\n",
    "        distractors_result = get_distractors(input_phrases, original_context, topn)\n",
    "        similar_keywords_list = [result[\"similar_keywords\"] for result in distractors_result]\n",
    "        return similar_keywords_list\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting similar keywords: {e}\")\n",
    "        return [[]]  # Return an empty list in case of error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mca_questions(context, qg_model, qg_tokenizer, sentence_transformer_model, num_questions=5, max_attempts=2) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Generate multiple-choice questions for a given context.\n",
    "    \n",
    "    Parameters:\n",
    "        context (str): The context from which questions are generated.\n",
    "        qg_model (T5ForConditionalGeneration): The question generation model.\n",
    "        qg_tokenizer (T5Tokenizer): The tokenizer for the question generation model.\n",
    "        s2v (Sense2Vec): The Sense2Vec model for finding similar words.\n",
    "        sentence_transformer_model (SentenceTransformer): The sentence transformer model for embeddings.\n",
    "        num_questions (int): The number of questions to generate.\n",
    "        max_attempts (int): The maximum number of attempts to generate questions.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of dictionaries with questions and their corresponding distractors.\n",
    "    \"\"\"\n",
    "    output_list = []\n",
    "\n",
    "    imp_keywords = get_keywords(context)\n",
    "    print(f\"[DEBUG] Length: {len(imp_keywords)}, Extracted keywords: {imp_keywords}\")\n",
    "\n",
    "    generated_questions = set()\n",
    "    generated_answers = set()\n",
    "    attempts = 0\n",
    "\n",
    "    while len(output_list) < num_questions and attempts < max_attempts:\n",
    "        attempts += 1\n",
    "\n",
    "        for keyword in imp_keywords:\n",
    "            if len(output_list) >= num_questions:\n",
    "                break\n",
    "            \n",
    "            question = get_question(context, keyword, qg_model, qg_tokenizer)\n",
    "            print(f\"[DEBUG] Generated question: '{question}' for keyword: '{keyword}'\")\n",
    "            \n",
    "            # Encode the new question\n",
    "            new_question_embedding = sentence_transformer_model.encode(question, convert_to_tensor=True)\n",
    "            is_similar = False\n",
    "\n",
    "            # Check similarity with existing questions\n",
    "            for generated_q in generated_questions:\n",
    "                existing_question_embedding = sentence_transformer_model.encode(generated_q, convert_to_tensor=True)\n",
    "                similarity = cosine_similarity(new_question_embedding.unsqueeze(0), existing_question_embedding.unsqueeze(0))[0][0]\n",
    "\n",
    "                if similarity > 0.8:\n",
    "                    is_similar = True\n",
    "                    print(f\"[DEBUG] Question '{question}' is too similar to an existing question, skipping.\")\n",
    "                    break\n",
    "\n",
    "            if is_similar:\n",
    "                continue\n",
    "\n",
    "            # Generate and check answer\n",
    "            t5_answer = answer_question(question, context)\n",
    "            print(f\"[DEBUG] Generated answer: '{t5_answer}' for question: '{question}'\")\n",
    "            \n",
    "            # Skip answers longer than 3 words\n",
    "            if len(t5_answer.split()) > 3:\n",
    "                print(f\"[DEBUG] Answer '{t5_answer}' is too long, skipping.\")\n",
    "                continue\n",
    "\n",
    "            if t5_answer in generated_answers:\n",
    "                print(f\"[DEBUG] Answer '{t5_answer}' has already been generated, skipping question.\")\n",
    "                continue\n",
    "\n",
    "            generated_questions.add(question)\n",
    "            generated_answers.add(t5_answer)\n",
    "\n",
    "            # Generate distractors\n",
    "            distractors = extract_similar_keywords([t5_answer], topn=5)[0]\n",
    "            print(f\"list of distractors : {distractors}\")\n",
    "            print(f\"length of distractors {len(distractors)}\")\n",
    "            print(f\"type : {type(distractors)}\")\n",
    "\n",
    "            # Remove any distractor that is the same as the correct answer\n",
    "            distractors = [d for d in distractors if d.lower() != t5_answer.lower()]\n",
    "            print(f\"Filtered distractors (without answer): {distractors}\")\n",
    "\n",
    "            # Ensure there are exactly 3 distractors\n",
    "            if len(distractors) < 3:\n",
    "                # Fill with random keywords from the imp_keywords list until we have 3 distractors\n",
    "                while len(distractors) < 3:\n",
    "                    random_keyword = random.choice(imp_keywords)\n",
    "                    # Ensure the random keyword isn't the same as the answer or already a distractor\n",
    "                    if random_keyword.lower() != t5_answer.lower() and random_keyword not in distractors:\n",
    "                        distractors.append(random_keyword)\n",
    "\n",
    "            # Limit to 3 distractors\n",
    "            distractors = distractors[:3]\n",
    "\n",
    "            print(f\"[DEBUG] Final distractors: {distractors} for question: '{question}'\")\n",
    "\n",
    "            choices = distractors + [t5_answer]\n",
    "            choices = [item.title() for item in choices]\n",
    "            random.shuffle(choices)\n",
    "            print(f\"[DEBUG] Options: {choices} for answer: '{t5_answer}'\")\n",
    "\n",
    "            # Classify question type\n",
    "            question_type = classify_question_type(question)\n",
    "            \n",
    "            output_list.append({\n",
    "                'answer': t5_answer,\n",
    "                'answer_length': len(t5_answer),\n",
    "                'choices': choices,\n",
    "                'passage': context,\n",
    "                'passage_length': len(context),\n",
    "                'question': question,\n",
    "                'question_length': len(question),\n",
    "                'question_type': question_type\n",
    "            })\n",
    "\n",
    "        print(f\"[DEBUG] Generated {len(output_list)} questions so far after {attempts} attempts\")\n",
    "\n",
    "    return output_list[:num_questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The creation of modern-day Eritrea is a result of the incorporation of independent, distinct kingdoms and sultanates (for example, Medri Bahri and the Sultanate of Aussa) eventually resulting in the formation of Italian Eritrea. In 1947 Eritrea became part of a federation with Ethiopia, the Federation of Ethiopia and Eritrea. Subsequent annexation into Ethiopia led to the Eritrean War of Independence, ending with Eritrean independence following a referendum in April 1993. Hostilities between Eritrea and Ethiopia persisted, leading to the Eritrean–Ethiopian War of 1998–2000 and further skirmishes with both Djibouti and Ethiopia.\n",
      "[DEBUG] Length: 41, Extracted keywords: ['eritrea', 'ethiopia', 'eritrean', 'federation', 'independence', 'war', '1947', '1993', '1998', '2000', 'annexation', 'april', 'aussa', 'bahri', 'creation', 'day', 'distinct', 'djibouti', 'ending', 'ethiopian', 'eventually', 'example', 'following', 'formation', 'hostilities', 'incorporation', 'independent', 'italian', 'kingdoms', 'leading', 'led', 'medri', 'modern', 'persisted', 'referendum', 'result', 'resulting', 'skirmishes', 'subsequent', 'sultanate', 'sultanates']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=200) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Generated question: 'Eritrea is a modern day Eritrean kingdom.' for keyword: 'eritrea'\n",
      "[DEBUG] Generated answer: 'Italian eritrea' for question: 'Eritrea is a modern day Eritrean kingdom.'\n",
      "Error extracting nouns: Cannot process input. It is neither a spacy doc, a string or a list of list of tuple: <class 'int'>\n",
      "[DEBUG] No suitable sense found for word: 'Italian eritrea'\n",
      "[DEBUG] Fallback to Multipartite keywords for 'Italian eritrea' due to insufficient distractors.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 2)) while a minimum of 1 is required by check_pairwise_arrays.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m original_context \u001b[38;5;241m=\u001b[39m get_passage(random_passage)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(original_context)\n\u001b[1;32m----> 5\u001b[0m questions_and_distractors \u001b[38;5;241m=\u001b[39m \u001b[43mget_mca_questions\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt5qg_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt5qg_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentence_transformer_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_questions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m OUTPUT_FILE \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplots\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_questions.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_to_serializable\u001b[39m(data):\n",
      "Cell \u001b[1;32mIn[26], line 70\u001b[0m, in \u001b[0;36mget_mca_questions\u001b[1;34m(context, qg_model, qg_tokenizer, sentence_transformer_model, num_questions, max_attempts)\u001b[0m\n\u001b[0;32m     67\u001b[0m generated_answers\u001b[38;5;241m.\u001b[39madd(t5_answer)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Generate distractors\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m distractors \u001b[38;5;241m=\u001b[39m \u001b[43mextract_similar_keywords\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt5_answer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlist of distractors : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdistractors\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength of distractors \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(distractors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[25], line 245\u001b[0m, in \u001b[0;36mextract_similar_keywords\u001b[1;34m(input_phrases, topn)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_similar_keywords\u001b[39m(input_phrases, topn\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m    244\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call get_distractors and extract only the similar_keywords values.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 245\u001b[0m     distractors_result \u001b[38;5;241m=\u001b[39m \u001b[43mget_distractors\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_phrases\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    247\u001b[0m     similar_keywords_list \u001b[38;5;241m=\u001b[39m [result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilar_keywords\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m distractors_result]\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m similar_keywords_list\n",
      "Cell \u001b[1;32mIn[25], line 233\u001b[0m, in \u001b[0;36mget_distractors\u001b[1;34m(input_phrases, content, topn)\u001b[0m\n\u001b[0;32m    230\u001b[0m         similar_keywords \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m imp_keywords\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;66;03m# Filter distractors to match word count, format, and semantic similarity\u001b[39;00m\n\u001b[1;32m--> 233\u001b[0m     final_distractors \u001b[38;5;241m=\u001b[39m \u001b[43mfilter_distractors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphrase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimilar_keywords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m     result_list\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m    236\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphrase\u001b[39m\u001b[38;5;124m\"\u001b[39m: phrase,\n\u001b[0;32m    237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilar_keywords\u001b[39m\u001b[38;5;124m\"\u001b[39m: final_distractors\n\u001b[0;32m    238\u001b[0m     })\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result_list\n",
      "Cell \u001b[1;32mIn[25], line 160\u001b[0m, in \u001b[0;36mfilter_distractors\u001b[1;34m(input_phrase, similar_keywords, topn)\u001b[0m\n\u001b[0;32m    151\u001b[0m valid_keywords \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    152\u001b[0m     keyword \u001b[38;5;28;01mfor\u001b[39;00m keyword \u001b[38;5;129;01min\u001b[39;00m similar_keywords\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(keyword\u001b[38;5;241m.\u001b[39msplit()) \u001b[38;5;241m==\u001b[39m word_count \u001b[38;5;129;01mand\u001b[39;00m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    156\u001b[0m     is_valid_distractor(keyword, input_phrase)\n\u001b[0;32m    157\u001b[0m ]\n\u001b[0;32m    159\u001b[0m phrases_to_compare \u001b[38;5;241m=\u001b[39m [input_phrase] \u001b[38;5;241m+\u001b[39m valid_keywords\n\u001b[1;32m--> 160\u001b[0m similarities \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphrases_to_compare\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, similarity \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(similarities):\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m0.3\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m similarity \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.8\u001b[39m:  \u001b[38;5;66;03m# Distractors should be relevant but not too close\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[25], line 141\u001b[0m, in \u001b[0;36mcalculate_similarity\u001b[1;34m(phrases)\u001b[0m\n\u001b[0;32m    139\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\u001b[38;5;241m.\u001b[39mfit_transform(phrases)\n\u001b[0;32m    140\u001b[0m vectors \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[1;32m--> 141\u001b[0m cosine_similarities \u001b[38;5;241m=\u001b[39m \u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvectors\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectors\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cosine_similarities[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32md:\\01SetupFiles\\AnacondaNavigator\\anacondanavigatorfiles\\envs\\Thesis\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32md:\\01SetupFiles\\AnacondaNavigator\\anacondanavigatorfiles\\envs\\Thesis\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:1657\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1613\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[0;32m   1614\u001b[0m \n\u001b[0;32m   1615\u001b[0m \u001b[38;5;124;03mCosine similarity, or the cosine kernel, computes similarity as the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1653\u001b[0m \u001b[38;5;124;03m       [0.57..., 0.81...]])\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m \u001b[38;5;66;03m# to avoid recursive import\u001b[39;00m\n\u001b[1;32m-> 1657\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_pairwise_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1659\u001b[0m X_normalized \u001b[38;5;241m=\u001b[39m normalize(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m Y:\n",
      "File \u001b[1;32md:\\01SetupFiles\\AnacondaNavigator\\anacondanavigatorfiles\\envs\\Thesis\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:172\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[1;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    164\u001b[0m     X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m    165\u001b[0m         X,\n\u001b[0;32m    166\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    170\u001b[0m         estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m    171\u001b[0m     )\n\u001b[1;32m--> 172\u001b[0m     Y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m precomputed:\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[1;32md:\\01SetupFiles\\AnacondaNavigator\\anacondanavigatorfiles\\envs\\Thesis\\Lib\\site-packages\\sklearn\\utils\\validation.py:1072\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1070\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[0;32m   1071\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[1;32m-> 1072\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1073\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1074\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1075\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[0;32m   1076\u001b[0m         )\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   1079\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 2)) while a minimum of 1 is required by check_pairwise_arrays."
     ]
    }
   ],
   "source": [
    "original_context = get_passage(random_passage)\n",
    "\n",
    "print(original_context)\n",
    "\n",
    "questions_and_distractors = get_mca_questions(original_context, t5qg_model, t5qg_tokenizer, sentence_transformer_model, num_questions=5)\n",
    "\n",
    "OUTPUT_FILE = os.path.join('..', 'outputs', 'plots', 'predictions', 'generated_questions.json')\n",
    "\n",
    "def convert_to_serializable(data):\n",
    "    if isinstance(data, pd.Series):\n",
    "        return data.tolist()\n",
    "    elif isinstance(data, dict):\n",
    "        return {k: convert_to_serializable(v) for k, v in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [convert_to_serializable(i) for i in data]\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "try:\n",
    "    with open(OUTPUT_FILE, 'r') as f:\n",
    "        existing_data = json.load(f)\n",
    "        if not isinstance(existing_data, list):  # Ensure existing data is a list\n",
    "            raise ValueError(\"The existing data is not a list.\")\n",
    "except FileNotFoundError:\n",
    "    existing_data = []\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Warning: {OUTPUT_FILE} contains invalid JSON. Initializing with an empty list.\")\n",
    "    existing_data = []\n",
    "except ValueError as e:\n",
    "    print(f\"ValueError: {e}\")\n",
    "    existing_data = []\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred while reading {OUTPUT_FILE}: {e}\")\n",
    "    existing_data = []\n",
    "\n",
    "# Convert any non-serializable data to a serializable format\n",
    "questions_and_distractors_serializable = convert_to_serializable(questions_and_distractors)\n",
    "existing_data.extend(questions_and_distractors_serializable)\n",
    "\n",
    "# Write updated data back to the file\n",
    "try:\n",
    "    with open(OUTPUT_FILE, 'w') as f:\n",
    "        json.dump(existing_data, f, indent=4)\n",
    "    print(f\"Data successfully written to {OUTPUT_FILE}.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while writing to {OUTPUT_FILE}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
