{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm\n",
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6tAzs7GjWUM",
        "outputId": "bc2424e9-70f8-4006-9a55-dceffaeb5556"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "159X0pPziSDI",
        "outputId": "3912f56e-0066-4dee-dc97-7220f18dfb2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "IYIo3OTwiVUK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_and_preprocess_text(text):\n",
        "    text = text.lower()\n",
        "\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "def preprocess_squad_data(filepath):\n",
        "    with open(filepath, 'r') as f:\n",
        "        squad_data = json.load(f)\n",
        "\n",
        "    preprocessed_data = []\n",
        "\n",
        "    for entry in tqdm(squad_data['data']):\n",
        "        for paragraph in entry['paragraphs']:\n",
        "            context = paragraph['context']\n",
        "            context_cleaned = clean_and_preprocess_text(context)\n",
        "\n",
        "            for qa in paragraph['qas']:\n",
        "                question = qa['question']\n",
        "                question_cleaned = clean_and_preprocess_text(question)\n",
        "\n",
        "                if qa['is_impossible']:\n",
        "                    continue\n",
        "\n",
        "                answer = qa['answers'][0]['text']\n",
        "                answer_cleaned = clean_and_preprocess_text(answer)\n",
        "\n",
        "                preprocessed_data.append({\n",
        "                    'context': context_cleaned,\n",
        "                    'question': question_cleaned,\n",
        "                    'answer': answer_cleaned\n",
        "                })\n",
        "\n",
        "    return preprocessed_data\n"
      ],
      "metadata": {
        "id": "468IEV2viYAc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "    squad_filepath = '/content/train-v2.0.json'\n",
        "\n",
        "\n",
        "    preprocessed_data = preprocess_squad_data(squad_filepath)\n",
        "\n",
        "\n",
        "    with open('preprocessed_squad_data.json', 'w') as f:\n",
        "        json.dump(preprocessed_data, f, indent=4)\n",
        "\n",
        "    print(\"Preprocessing of SQuAD 2.0 dataset completed!\")\n"
      ],
      "metadata": {
        "id": "QkU4OtXLiZ8p"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEvj7TM8ibYF",
        "outputId": "7bb8fdff-7bd4-4fd5-f997-a0f055170ef3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 442/442 [00:59<00:00,  7.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing of SQuAD 2.0 dataset completed!\n"
          ]
        }
      ]
    }
  ]
}