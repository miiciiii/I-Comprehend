{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\02 Personal Files\\Thesis Related\\Main Thesis Project\\main\\thesis\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\justi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\justi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\justi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\justi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\justi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\", message=\"This sequence already has </s>.\")\n",
    "\n",
    "\n",
    "# Standard library imports\n",
    "import random\n",
    "import string\n",
    "\n",
    "# Third-party imports\n",
    "import nltk\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pke\n",
    "import torch\n",
    "from nltk.corpus import stopwords, wordnet as wn\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sense2vec import Sense2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from textdistance import levenshtein\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('brown')\n",
    "nltk.download('wordnet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File Paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RANDOM_PASSAGE_PATH = os.path.join('datasets', 'generated_qa.csv')\n",
    "T5QG_MODEL_DIR = os.path.join('models', 'qg_model')\n",
    "T5QG_TOKENIZER_DIR = os.path.join('models', 'qg_tokenizer')\n",
    "T5AG_MODEL_DIR = os.path.join('models', 't5_model')\n",
    "T5AG_TOKENIZER_DIR = os.path.join('models', 't5_tokenizer')\n",
    "S2V_MODEL_PATH = os.path.join('models', 's2v_old')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preload Models and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "random_passage = pd.read_csv(RANDOM_PASSAGE_PATH)\n",
    "t5ag_model = T5ForConditionalGeneration.from_pretrained(T5AG_MODEL_DIR)\n",
    "t5ag_tokenizer = T5Tokenizer.from_pretrained(T5AG_TOKENIZER_DIR)\n",
    "t5qg_model = T5ForConditionalGeneration.from_pretrained(T5QG_MODEL_DIR)\n",
    "t5qg_tokenizer = T5Tokenizer.from_pretrained(T5QG_TOKENIZER_DIR)\n",
    "s2v = Sense2Vec().from_disk(S2V_MODEL_PATH)\n",
    "sentence_transformer_model = SentenceTransformer(\"sentence-transformers/LaBSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, context):\n",
    "    \"\"\"Generate an answer for a given question and context.\"\"\"\n",
    "    input_text = f\"question: {question} context: {context}\"\n",
    "    input_ids = t5ag_tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = t5ag_model.generate(input_ids, max_length=512, num_return_sequences=1, max_new_tokens=200)\n",
    "\n",
    "    return t5ag_tokenizer.decode(output[0], skip_special_tokens=True).capitalize()\n",
    "\n",
    "\n",
    "def get_nouns_multipartite(content):\n",
    "    \"\"\"Extract keywords from content using MultipartiteRank algorithm.\"\"\"\n",
    "    try:\n",
    "        extractor = pke.unsupervised.MultipartiteRank()\n",
    "        extractor.load_document(input=content, language='en')\n",
    "        pos_tags = {'PROPN', 'NOUN', 'ADJ', 'VERB', 'ADP', 'ADV', 'DET', 'CONJ', 'NUM', 'PRON', 'X'}\n",
    "\n",
    "        stoplist = list(string.punctuation) + ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']\n",
    "        stoplist += stopwords.words('english')\n",
    "\n",
    "        extractor.candidate_selection(pos=pos_tags)\n",
    "        extractor.candidate_weighting(alpha=1.1, threshold=0.75, method='average')\n",
    "        keyphrases = extractor.get_n_best(n=15)\n",
    "        \n",
    "        return [val[0] for val in keyphrases]\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting nouns: {e}\")\n",
    "        return []\n",
    "\n",
    "    \n",
    "def get_keywords(passage):\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    \n",
    "    tfidf_matrix = vectorizer.fit_transform([passage])\n",
    "    \n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    tfidf_scores = tfidf_matrix.toarray().flatten()\n",
    "    \n",
    "    word_scores = dict(zip(feature_names, tfidf_scores))\n",
    "\n",
    "    sorted_words = sorted(word_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    keywords = [word for word, score in sorted_words]\n",
    "    \n",
    "    return keywords\n",
    "\n",
    "\n",
    "def get_question(context, answer, model, tokenizer):\n",
    "    \"\"\"Generate a question for the given answer and context.\"\"\"\n",
    "    answer_span = context.replace(answer, f\"<hl>{answer}<hl>\", 1) + \"</s>\"\n",
    "    inputs = tokenizer(answer_span, return_tensors=\"pt\")\n",
    "    question = model.generate(input_ids=inputs.input_ids, max_length=50)[0]\n",
    "\n",
    "    return tokenizer.decode(question, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def filter_same_sense_words(original, wordlist):\n",
    "    \"\"\"Filter words that have the same sense as the original word.\"\"\"\n",
    "    base_sense = original.split('|')[1]\n",
    "    return [word[0].split('|')[0].replace(\"_\", \" \").title().strip() for word in wordlist if word[0].split('|')[1] == base_sense]\n",
    "\n",
    "def get_max_similarity_score(wordlist, word):\n",
    "    \"\"\"Get the maximum similarity score between the word and a list of words.\"\"\"\n",
    "    return max(levenshtein.normalized_similarity(word.lower(), each.lower()) for each in wordlist)\n",
    "\n",
    "def sense2vec_get_words(word, s2v, topn, question):\n",
    "    \"\"\"Get similar words using Sense2Vec.\"\"\"\n",
    "    try:\n",
    "        sense = s2v.get_best_sense(word, senses=[\"NOUN\", \"PERSON\", \"PRODUCT\", \"LOC\", \"ORG\", \"EVENT\", \"NORP\", \"WORK_OF_ART\", \"FAC\", \"GPE\", \"NUM\", \"FACILITY\"])\n",
    "        \n",
    "        if sense is None:\n",
    "            print(f\"[DEBUG] No suitable sense found for word: '{word}'\")\n",
    "            return []\n",
    "\n",
    "        most_similar = s2v.most_similar(sense, n=topn)\n",
    "        output = filter_same_sense_words(sense, most_similar)\n",
    "        \n",
    "        threshold = 0.6\n",
    "        final = [word]\n",
    "        checklist = question.split()\n",
    "\n",
    "        for similar_word in output:\n",
    "            if get_max_similarity_score(final, similar_word) < threshold and similar_word not in final and similar_word not in checklist:\n",
    "                final.append(similar_word)\n",
    "    \n",
    "        return final[1:]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in Sense2Vec: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def mmr(doc_embedding, word_embeddings, words, top_n, lambda_param):\n",
    "    \"\"\"Maximal Marginal Relevance (MMR) for keyword extraction.\"\"\"\n",
    "    word_doc_similarity = cosine_similarity(word_embeddings, doc_embedding)\n",
    "    word_similarity = cosine_similarity(word_embeddings)\n",
    "\n",
    "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "    for _ in range(top_n - 1):\n",
    "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "\n",
    "        mmr = (lambda_param * candidate_similarities) - ((1 - lambda_param) * target_similarities.reshape(-1, 1))\n",
    "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "\n",
    "        keywords_idx.append(mmr_idx)\n",
    "        candidates_idx.remove(mmr_idx)\n",
    "\n",
    "    return [words[idx] for idx in keywords_idx]\n",
    "\n",
    "def get_distractors_wordnet(word):\n",
    "    \"\"\"Get distractors using WordNet.\"\"\"\n",
    "    distractors = []\n",
    "    try:\n",
    "        synset = wn.synsets(word, 'n')[0]\n",
    "        hypernym = synset.hypernyms()\n",
    "        if not hypernym:\n",
    "            return distractors\n",
    "        for item in hypernym[0].hyponyms():\n",
    "            name = item.lemmas()[0].name().replace(\"_\", \" \").title()\n",
    "            if name.lower() != word.lower() and name not in distractors:\n",
    "                distractors.append(name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in WordNet distractors: {e}\")\n",
    "        pass\n",
    "\n",
    "    return distractors\n",
    "\n",
    "def get_distractors(word, original_sentence, sense2vec_model, sentence_model, top_n, lambda_val):\n",
    "    \"\"\"Get distractors for a given word using various methods.\"\"\"\n",
    "    distractors = sense2vec_get_words(word, sense2vec_model, top_n, original_sentence)\n",
    "    if not distractors:\n",
    "        return []\n",
    "\n",
    "    distractors_new = [word.capitalize()] + distractors\n",
    "    embedding_sentence = f\"{original_sentence} {word.capitalize()}\"\n",
    "    keyword_embedding = sentence_model.encode([embedding_sentence])\n",
    "    distractor_embeddings = sentence_model.encode(distractors_new)\n",
    "\n",
    "    max_keywords = min(len(distractors_new), 5)\n",
    "    filtered_keywords = mmr(keyword_embedding, distractor_embeddings, distractors_new, max_keywords, lambda_val)\n",
    "    return [kw.capitalize() for kw in filtered_keywords if kw.lower() != word.lower()][1:]\n",
    "\n",
    "\n",
    "def get_mca_questions(context, qg_model, qg_tokenizer, s2v, sentence_transformer_model, num_questions=5, max_attempts=2):\n",
    "    \"\"\"\n",
    "    Generate multiple-choice questions for a given context.\n",
    "    \n",
    "    Parameters:\n",
    "        context (str): The context from which questions are generated.\n",
    "        qg_model (T5ForConditionalGeneration): The question generation model.\n",
    "        qg_tokenizer (T5Tokenizer): The tokenizer for the question generation model.\n",
    "        s2v (Sense2Vec): The Sense2Vec model for finding similar words.\n",
    "        sentence_transformer_model (SentenceTransformer): The sentence transformer model for embeddings.\n",
    "        num_questions (int): The number of questions to generate.\n",
    "        max_attempts (int): The maximum number of attempts to generate questions.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of dictionaries with questions and their corresponding distractors.\n",
    "    \"\"\"\n",
    "    output_list = []\n",
    "\n",
    "\n",
    "    imp_keywords = get_keywords(context)  # Extract keywords only once\n",
    "    print(f\"[DEBUG] Extracted keywords: {imp_keywords}, length: {len(imp_keywords)}\")\n",
    "\n",
    "    generated_questions = set()\n",
    "    attempts = 0\n",
    "\n",
    "    lengths = random_passage.iloc[:, 0].str.len()\n",
    "\n",
    "    lowest_length = lengths.min()\n",
    "    highest_length = lengths.max()\n",
    "\n",
    "\n",
    "    length_range = highest_length - lowest_length\n",
    "    difficulty_range = length_range / 3\n",
    "\n",
    "    difficulty_easy = lowest_length + difficulty_range\n",
    "    difficulty_medium = difficulty_easy + difficulty_range\n",
    "\n",
    "\n",
    "    difficulty = \"\"\n",
    "\n",
    "    if len(context) >= lowest_length and len(context) <= difficulty_easy:\n",
    "        difficulty = \"easy\"\n",
    "    elif len(context) >= difficulty_easy and len(context) <= difficulty_medium:\n",
    "        difficulty = \"medium\"\n",
    "    elif len(context) >= difficulty_medium and len(context) <= highest_length:\n",
    "        difficulty = \"hard\"\n",
    "\n",
    "\n",
    "    while len(output_list) < num_questions and attempts < max_attempts:\n",
    "        attempts += 1\n",
    "\n",
    "        for keyword in imp_keywords:\n",
    "            if len(output_list) >= num_questions:\n",
    "                break\n",
    "            \n",
    "            question = get_question(context, keyword, qg_model, qg_tokenizer)\n",
    "            print(f\"[DEBUG] Generated question: '{question}' for keyword: '{keyword}'\")\n",
    "            \n",
    "            # Encode the new question\n",
    "            new_question_embedding = sentence_transformer_model.encode(question, convert_to_tensor=True)\n",
    "            is_similar = False\n",
    "\n",
    "            # Check similarity with existing questions\n",
    "            for generated_q in generated_questions:\n",
    "                existing_question_embedding = sentence_transformer_model.encode(generated_q, convert_to_tensor=True)\n",
    "                similarity = cosine_similarity(new_question_embedding.unsqueeze(0), existing_question_embedding.unsqueeze(0))[0][0]\n",
    "\n",
    "                if similarity > 0.8:\n",
    "                    is_similar = True\n",
    "                    print(f\"[DEBUG] Question '{question}' is too similar to an existing question, skipping.\")\n",
    "                    break\n",
    "\n",
    "            if is_similar:\n",
    "                continue\n",
    "\n",
    "            generated_questions.add(question)\n",
    "\n",
    "            t5_answer = answer_question(question, context)\n",
    "            print(f\"[DEBUG] Generated answer: '{t5_answer}' for question: '{question}'\")\n",
    "\n",
    "            distractors = get_distractors(t5_answer.capitalize(), question, s2v, sentence_transformer_model, 40, 0.2)\n",
    "            print(f\"[DEBUG] Generated distractors: {distractors} for question: '{question}'\")\n",
    "\n",
    "            if len(distractors) == 0:\n",
    "                print(\"[DEBUG] No distractors found, using important keywords as distractors.\")\n",
    "                distractors = imp_keywords\n",
    "\n",
    "            distractors = [d.capitalize() for d in distractors if d.lower() != keyword.lower()]\n",
    "\n",
    "            if len(distractors) < 3:\n",
    "                additional_distractors = [kw.capitalize() for kw in imp_keywords if kw.capitalize() not in distractors and kw.lower() != keyword.lower()]\n",
    "                distractors.extend(additional_distractors[:3 - len(distractors)])\n",
    "            else:\n",
    "                distractors = distractors[:3]\n",
    "\n",
    "            print(f\"[DEBUG] Final distractors: {distractors} for question: '{question}'\")\n",
    "\n",
    "            choices = distractors + [t5_answer]\n",
    "            choices = [item.title() for item in choices]\n",
    "            random.shuffle(choices)\n",
    "            print(f\"[DEBUG] Options: {choices} for question: '{question}'\")\n",
    "\n",
    "            output_list.append({\n",
    "                'difficulty' : difficulty,\n",
    "                'passage' : context,\n",
    "                'questions-choices-answer': {\n",
    "                    'question' : question,\n",
    "                    'choices' : choices,\n",
    "                    'answer' : t5_answer\n",
    "                },\n",
    "            })\n",
    "\n",
    "        print(f\"[DEBUG] Generated {len(output_list)} questions so far after {attempts} attempts\")\n",
    "\n",
    "    return output_list[:num_questions]\n",
    "\n",
    "\n",
    "def get_passage(passage):\n",
    "    \"\"\"Generate a random context from the dataset.\"\"\"\n",
    "    return passage.sample(n=1)['context'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Model Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As this process continued, the missile found itself being used for more and more of the roles formerly filled by guns. First to go were the large weapons, replaced by equally large missile systems of much higher performance. Smaller missiles soon followed, eventually becoming small enough to be mounted on armored cars and tank chassis. These started replacing, or at least supplanting, similar gun-based SPAAG systems in the 1960s, and by the 1990s had replaced almost all such systems in modern armies. Man-portable missiles, MANPADs as they are known today, were introduced in the 1960s and have supplanted or even replaced even the smallest guns in most advanced armies.\n",
      "[DEBUG] Extracted keywords: ['replaced', 'systems', '1960s', 'armies', 'guns', 'large', 'missile', 'missiles', '1990s', 'advanced', 'armored', 'based', 'cars', 'chassis', 'continued', 'equally', 'eventually', 'filled', 'followed', 'gun', 'higher', 'introduced', 'known', 'man', 'manpads', 'modern', 'mounted', 'performance', 'portable', 'process', 'replacing', 'roles', 'similar', 'small', 'smaller', 'smallest', 'soon', 'spaag', 'started', 'supplanted', 'supplanting', 'tank', 'today', 'used', 'weapons'], length: 45\n",
      "[DEBUG] Generated question: 'What happened to the large weapons?' for keyword: 'replaced'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=200) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Generated answer: 'Replaced by equally large missile systems of much higher performance' for question: 'What happened to the large weapons?'\n",
      "[DEBUG] No suitable sense found for word: 'Replaced by equally large missile systems of much higher performance'\n",
      "[DEBUG] Generated distractors: [] for question: 'What happened to the large weapons?'\n",
      "[DEBUG] No distractors found, using important keywords as distractors.\n",
      "[DEBUG] Final distractors: ['Systems', '1960s', 'Armies'] for question: 'What happened to the large weapons?'\n",
      "[DEBUG] Options: ['1960S', 'Systems', 'Replaced By Equally Large Missile Systems Of Much Higher Performance', 'Armies'] for question: 'What happened to the large weapons?'\n",
      "[DEBUG] Generated question: 'What replaced the large weapons?' for keyword: 'systems'\n",
      "[DEBUG] Question 'What replaced the large weapons?' is too similar to an existing question, skipping.\n",
      "[DEBUG] Generated question: 'When did MANPADs start replacing gun-based SPAAG systems?' for keyword: '1960s'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=200) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Generated answer: 'The 1960s' for question: 'When did MANPADs start replacing gun-based SPAAG systems?'\n",
      "[DEBUG] No suitable sense found for word: 'The 1960s'\n",
      "[DEBUG] Generated distractors: [] for question: 'When did MANPADs start replacing gun-based SPAAG systems?'\n",
      "[DEBUG] No distractors found, using important keywords as distractors.\n",
      "[DEBUG] Final distractors: ['Replaced', 'Systems', 'Armies'] for question: 'When did MANPADs start replacing gun-based SPAAG systems?'\n",
      "[DEBUG] Options: ['Replaced', 'The 1960S', 'Armies', 'Systems'] for question: 'When did MANPADs start replacing gun-based SPAAG systems?'\n",
      "[DEBUG] Generated question: 'In what modern system had MANPADs replaced most of the SPAAG systems?' for keyword: 'armies'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=200) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Generated answer: 'Modern armies' for question: 'In what modern system had MANPADs replaced most of the SPAAG systems?'\n",
      "[DEBUG] Generated distractors: ['Bolt-action rifles', 'Main battle tanks', 'Urban combat'] for question: 'In what modern system had MANPADs replaced most of the SPAAG systems?'\n",
      "[DEBUG] Final distractors: ['Bolt-action rifles', 'Main battle tanks', 'Urban combat'] for question: 'In what modern system had MANPADs replaced most of the SPAAG systems?'\n",
      "[DEBUG] Options: ['Main Battle Tanks', 'Modern Armies', 'Urban Combat', 'Bolt-Action Rifles'] for question: 'In what modern system had MANPADs replaced most of the SPAAG systems?'\n",
      "[DEBUG] Generated question: 'What was the SPAAG system formerly used for?' for keyword: 'guns'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=200) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Generated answer: 'Manpads' for question: 'What was the SPAAG system formerly used for?'\n",
      "[DEBUG] Generated distractors: ['Anti-tank weapons', 'Ifvs', 'Sead'] for question: 'What was the SPAAG system formerly used for?'\n",
      "[DEBUG] Final distractors: ['Anti-tank weapons', 'Ifvs', 'Sead'] for question: 'What was the SPAAG system formerly used for?'\n",
      "[DEBUG] Options: ['Sead', 'Manpads', 'Ifvs', 'Anti-Tank Weapons'] for question: 'What was the SPAAG system formerly used for?'\n",
      "[DEBUG] Generated question: 'What type of weapons were first to go?' for keyword: 'large'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=200) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Generated answer: 'Large weapons' for question: 'What type of weapons were first to go?'\n",
      "[DEBUG] Generated distractors: ['Plate armor', 'Javelins', 'Longswords'] for question: 'What type of weapons were first to go?'\n",
      "[DEBUG] Final distractors: ['Plate armor', 'Javelins', 'Longswords'] for question: 'What type of weapons were first to go?'\n",
      "[DEBUG] Options: ['Plate Armor', 'Longswords', 'Javelins', 'Large Weapons'] for question: 'What type of weapons were first to go?'\n",
      "[DEBUG] Generated 5 questions so far after 1 attempts\n",
      "Generated 5 questions. Saved to generated_questions.json\n"
     ]
    }
   ],
   "source": [
    "original_context = get_passage(random_passage)\n",
    "\n",
    "print(original_context)\n",
    "\n",
    "questions_and_distractors = get_mca_questions(original_context, t5qg_model, t5qg_tokenizer, s2v, sentence_transformer_model, num_questions=5)\n",
    "\n",
    "# Save to JSON file\n",
    "output_file = \"generated_questions.json\"\n",
    "try:\n",
    "    with open(output_file, 'r') as f:\n",
    "        existing_data = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    existing_data = []\n",
    "\n",
    "existing_data.extend(questions_and_distractors)\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(existing_data, f, indent=4)\n",
    "\n",
    "print(f\"Generated {len(questions_and_distractors)} questions. Saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
