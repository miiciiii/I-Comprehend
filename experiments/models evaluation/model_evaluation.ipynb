{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\justi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\justi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard Library Imports\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"This sequence already has </s>.\")\n",
    "\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# Third-Party Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.util import ngrams\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedTokenizerFast,\n",
    "    AutoModelForQuestionAnswering\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "\n",
    "T5QG_MODEL_DIR = os.path.join('models', 'qg_model')\n",
    "T5QG_TOKENIZER_DIR = os.path.join('models', 'qg_tokenizer')\n",
    "T5AG_MODEL_DIR = os.path.join('models', 't5_model')\n",
    "T5AG_TOKENIZER_DIR = os.path.join('models', 't5_tokenizer')\n",
    "\n",
    "# dataset\n",
    "\n",
    "dataset_path = os.path.join('datasets', 'generated_qa.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Answer Generation Models\n",
    "agmodel1 = T5ForConditionalGeneration.from_pretrained(T5AG_MODEL_DIR) # valhalla/t5-base-qg-hl (finetuned)\n",
    "agmodel1_tokenizer = T5Tokenizer.from_pretrained(T5AG_TOKENIZER_DIR)\n",
    "\n",
    "agmodel2 = AutoModelForQuestionAnswering.from_pretrained('deepset/roberta-base-squad2')\n",
    "agmodel2_tokenizer = AutoTokenizer.from_pretrained('deepset/roberta-base-squad2')\n",
    "\n",
    "agmodel3 = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "agmodel3_tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "    \n",
    "\"\"\" =============================================================== \"\"\"\n",
    "\n",
    "# Question Generation Models\n",
    "qgmodel1 = T5ForConditionalGeneration.from_pretrained(T5QG_MODEL_DIR) # t5-large (default)\n",
    "qgmodel1_tokenizer = T5Tokenizer.from_pretrained(T5QG_TOKENIZER_DIR)\n",
    "\n",
    "qgmodel2 = T5ForConditionalGeneration.from_pretrained(\"iarfmoose/t5-base-question-generator\")\n",
    "qgmodel2_tokenizer = T5Tokenizer.from_pretrained(\"iarfmoose/t5-base-question-generator\")\n",
    "\n",
    "qgmodel3 = T5ForConditionalGeneration.from_pretrained('Sehong/t5-large-QuestionGeneration')\n",
    "qgmodel3_tokenizer = PreTrainedTokenizerFast.from_pretrained('Sehong/t5-large-QuestionGeneration')\n",
    "\n",
    "\"\"\" =============================================================== \"\"\"\n",
    "\n",
    "# Sentence Transformer Models\n",
    "stmodel1 = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "stmodel2 = SentenceTransformer(\"sentence-transformers/msmarco-distilbert-base-v2\")\n",
    "\n",
    "stmodel3 = SentenceTransformer('sentence-transformers/LaBSE') # sentence-transformers/LaBSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "nlength = 100 # 1000 (higher much better)\n",
    "\n",
    "df_sample = df.sample(n = nlength, random_state=42)\n",
    "\n",
    "ref_contexts = df_sample['context'].tolist()\n",
    "ref_answers = df_sample['answer'].tolist()\n",
    "bleu_ref_answers = [[ref] for ref in ref_answers]\n",
    "rouge_ref = [[ref] for ref in ref_answers]\n",
    "meteor_ref = [[ref] for ref in ref_answers]\n",
    "ref_questions = df_sample['question'].tolist()\n",
    "bleu_ref_questions = [[ref] for ref in ref_questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_answer_generation_model1(context, question):\n",
    "    input_text = f\"question: {question} context: {context}\"\n",
    "    input_ids = agmodel1_tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = agmodel1.generate(input_ids, max_length=512, num_return_sequences=1)\n",
    "\n",
    "    return agmodel1_tokenizer.decode(output[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Predictions for Answer Generation Model 1: 100%|██████████| 100/100 [01:28<00:00,  1.13it/s]\n"
     ]
    }
   ],
   "source": [
    "answer_generation_model1_hyp = []\n",
    "\n",
    "with tqdm(total=len(ref_answers), desc=\"Generating Predictions for Answer Generation Model 1\") as pbar:\n",
    "    for answer, question, context in zip(ref_answers, ref_questions, ref_contexts):\n",
    "        outputs = encode_answer_generation_model1(context, question)\n",
    "        answer_generation_model1_hyp.append(outputs)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_answer_generation_model2(context, question):\n",
    "    input_text = question + agmodel2_tokenizer.sep_token + context\n",
    "\n",
    "    input_text = input_text[:agmodel2_tokenizer.model_max_length]\n",
    "\n",
    "    encoding = agmodel2_tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    outputs = agmodel2(**encoding)\n",
    "\n",
    "    predicted_start_idx = outputs.start_logits.argmax(-1).item()\n",
    "    predicted_end_idx = outputs.end_logits.argmax(-1).item()\n",
    "    predicted_answer = agmodel2_tokenizer.decode(encoding.input_ids.squeeze()[predicted_start_idx : predicted_end_idx + 1])\n",
    "\n",
    "    return predicted_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Predictions for Answer Generation Model 2: 100%|██████████| 100/100 [00:19<00:00,  5.26it/s]\n"
     ]
    }
   ],
   "source": [
    "answer_generation_model2_hyp = []\n",
    "\n",
    "for context, question in tqdm(zip(ref_contexts, ref_questions), desc = \"Generating Predictions for Answer Generation Model 2\", total=len(df_sample)):\n",
    "    answer = encode_answer_generation_model2(context, question)\n",
    "    answer_generation_model2_hyp.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_answer_generation_model3(context, question):\n",
    "    inputs = agmodel3_tokenizer(question, context, truncation=True, max_length=512, return_tensors='pt')\n",
    "    \n",
    "    inputs = {k: v.to(agmodel3.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = agmodel3(**inputs)\n",
    "    \n",
    "    start_scores = outputs.start_logits\n",
    "    end_scores = outputs.end_logits\n",
    "    start_index = torch.argmax(start_scores)\n",
    "    end_index = torch.argmax(end_scores)\n",
    "    \n",
    "    answer = agmodel3_tokenizer.convert_tokens_to_string(agmodel3_tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][start_index:end_index+1]))\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Predictions for Answer Generation Model 3: 100%|██████████| 100/100 [01:10<00:00,  1.41it/s]\n"
     ]
    }
   ],
   "source": [
    "answer_generation_model3_hyp = []\n",
    "\n",
    "for context, question in tqdm(zip(ref_contexts, ref_questions), desc=\"Generating Predictions for Answer Generation Model 3\", total=len(df_sample)):\n",
    "    answer = encode_answer_generation_model3(context, question)\n",
    "    answer_generation_model3_hyp.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_question_generation_model1(context, answer):\n",
    "    answer_span = context.replace(answer, f\"<hl>{answer}<hl>\") + \"</s>\"\n",
    "    inputs = qgmodel1_tokenizer(answer_span, return_tensors=\"pt\")\n",
    "    question = qgmodel1.generate(input_ids=inputs.input_ids, max_length=50)[0]\n",
    "\n",
    "    return qgmodel1_tokenizer.decode(question, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Predictions for Question Generation Model 1:  85%|████████▌ | 85/100 [01:21<00:13,  1.14it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (970 > 512). Running this sequence through the model will result in indexing errors\n",
      "Generating Predictions for Question Generation Model 1: 100%|██████████| 100/100 [01:40<00:00,  1.01s/it]\n"
     ]
    }
   ],
   "source": [
    "question_generation_model1_hyp = []\n",
    "\n",
    "for context, answer in tqdm(zip(ref_contexts, ref_answers), desc=\"Generating Predictions for Question Generation Model 1\", total=len(df_sample)):\n",
    "    question = encode_question_generation_model1(context, answer)\n",
    "    question_generation_model1_hyp.append(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_question_generation_model2(context):\n",
    "    input_text = f\"Generate a question from the context: {context} Answer: {answer}\"\n",
    "        \n",
    "    inputs = qgmodel3_tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    outputs = qgmodel3.generate(inputs, max_length=512, num_beams= 5, early_stopping=True)\n",
    "        \n",
    "    question = qgmodel3_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "    return question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Predictions for Question Generation Model 2: 100%|██████████| 100/100 [08:25<00:00,  5.06s/it]\n"
     ]
    }
   ],
   "source": [
    "question_generation_model2_hyp = []\n",
    "\n",
    "for context in tqdm(zip(ref_contexts), desc=\"Generating Predictions for Question Generation Model 2\", total=len(df_sample)):\n",
    "    question = encode_question_generation_model2(context)\n",
    "    question_generation_model2_hyp.append(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_question_generation_model3(context, answer):\n",
    "    input_text = f\"question: context: {context} answer: {answer}\"\n",
    "    inputs = qgmodel3_tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    outputs = qgmodel3.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=50,\n",
    "        num_beams=5,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    question = qgmodel3_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Predictions for Question Generation Model 3: 100%|██████████| 100/100 [10:21<00:00,  6.22s/it]\n"
     ]
    }
   ],
   "source": [
    "question_generation_model3_hyp = []\n",
    "\n",
    "for context, answer in tqdm(zip(ref_contexts, ref_answers), desc=\"Generating Predictions for Question Generation Model 3\", total=len(df_sample)):\n",
    "    question = encode_question_generation_model3(context, answer)\n",
    "    question_generation_model3_hyp.append(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sentence_transformers(ref_contexts):\n",
    "    embeddings1 = stmodel1.encode(ref_contexts)\n",
    "    embeddings2 = stmodel2.encode(ref_contexts)\n",
    "    embeddings3 = stmodel3.encode(ref_contexts)\n",
    "\n",
    "    similarity_matrix1 = cosine_similarity(embeddings1)\n",
    "    similarity_matrix2 = cosine_similarity(embeddings2)\n",
    "    similarity_matrix3 = cosine_similarity(embeddings3)\n",
    "\n",
    "    return {\n",
    "        \"Model1 (all-MiniLM-L6-v2)\": similarity_matrix1,\n",
    "        \"Model2 (msmarco-distilbert-base-v2)\": similarity_matrix2,\n",
    "        \"Model3 (LaBSE)\": similarity_matrix3\n",
    "    }\n",
    "\n",
    "def average_cosine_similarity(cos_sim_matrix):\n",
    "    avg_cos_sim = np.mean(cos_sim_matrix)\n",
    "    avg_cos_sim_percentage = avg_cos_sim * 100\n",
    "    return avg_cos_sim_percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics Formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLEU Metric\n",
    "\n",
    "def compute_bleu_score(reference, hypothesis, n=4):\n",
    "    def _ngram_counts(tokens, n):\n",
    "        return Counter(ngrams(tokens, n))\n",
    "\n",
    "    def _precision(hyp_counts, ref_counts):\n",
    "        return sum((hyp_counts & ref_counts).values()) / sum(hyp_counts.values()) if sum(hyp_counts.values()) > 0 else 0.0\n",
    "\n",
    "    def _brevity_penalty(ref_len, hyp_len):\n",
    "        if hyp_len > ref_len:\n",
    "            return 1.0\n",
    "        return math.exp(1 - ref_len / hyp_len) if hyp_len > 0 else 0.0\n",
    "\n",
    "    def _normalize(tokens):\n",
    "        return [token.lower() for token in tokens]\n",
    "\n",
    "    hyp_tokens = _normalize(hypothesis)\n",
    "    ref_tokens_list = [_normalize(ref) for ref in reference]\n",
    "    \n",
    "    precisions = []\n",
    "    for i in range(1, n+1):\n",
    "        hyp_ngrams = _ngram_counts(hyp_tokens, i)\n",
    "        ref_ngrams_list = [ _ngram_counts(ref, i) for ref in ref_tokens_list ]\n",
    "        \n",
    "        ref_ngrams = Counter()\n",
    "        for ref_ngram in ref_ngrams_list:\n",
    "            ref_ngrams |= ref_ngram\n",
    "\n",
    "        precision = _precision(hyp_ngrams, ref_ngrams)\n",
    "        precisions.append(precision)\n",
    "\n",
    "    if any(p == 0 for p in precisions):\n",
    "        return 0.0\n",
    "\n",
    "    bleu_score = math.exp(sum(math.log(p) for p in precisions) / n)\n",
    "    bp = _brevity_penalty(len(ref_tokens_list[0]), len(hyp_tokens))\n",
    "    return bp * bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUGE Metrics\n",
    "\n",
    "def count_ngrams(text, n):\n",
    "    ngrams = Counter()\n",
    "    words = text.split()\n",
    "    for i in range(len(words) - n + 1):\n",
    "        ngram = tuple(words[i:i+n])\n",
    "        ngrams[ngram] += 1\n",
    "    return ngrams\n",
    "\n",
    "def rouge_n(candidate, reference, n):\n",
    "    candidate_ngrams = count_ngrams(candidate, n)\n",
    "    reference_ngrams = count_ngrams(reference, n)\n",
    "    \n",
    "    match_count = 0\n",
    "    for ngram in reference_ngrams:\n",
    "        if ngram in candidate_ngrams:\n",
    "            match_count += min(reference_ngrams[ngram], candidate_ngrams[ngram])\n",
    "    \n",
    "    if sum(reference_ngrams.values()) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return match_count / sum(reference_ngrams.values())\n",
    "\n",
    "def lcs(X, Y):\n",
    "    m, n = len(X), len(Y)\n",
    "    table = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            if X[i] == Y[j]:\n",
    "                table[i + 1][j + 1] = table[i][j] + 1\n",
    "            else:\n",
    "                table[i + 1][j + 1] = max(table[i + 1][j], table[i][j + 1])\n",
    "    return table[m][n]\n",
    "\n",
    "def rouge_l(candidate, reference):\n",
    "    candidate_words = candidate.split()\n",
    "    reference_words = reference.split()\n",
    "    lcs_length = lcs(candidate_words, reference_words)\n",
    "    \n",
    "    if len(candidate_words) == 0 or len(reference_words) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    precision = lcs_length / len(candidate_words)\n",
    "    recall = lcs_length / len(reference_words)\n",
    "    \n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    f1_score = (2 * precision * recall) / (precision + recall)\n",
    "    return f1_score\n",
    "\n",
    "def calculate_rouge_scores(candidates, references):\n",
    "    assert len(candidates) == len(references), \"Candidates and references must be of the same length\"\n",
    "    \n",
    "    rouge_1_scores = []\n",
    "    rouge_2_scores = []\n",
    "    for candidate, reference in zip(candidates, references):\n",
    "        rouge_1_scores.append(rouge_n(candidate, reference, 1))\n",
    "        rouge_2_scores.append(rouge_n(candidate, reference, 2))\n",
    "    \n",
    "    rouge_l_scores = []\n",
    "    for candidate, reference in zip(candidates, references):\n",
    "        rouge_l_scores.append(rouge_l(candidate, reference))\n",
    "    \n",
    "    avg_rouge_1 = sum(rouge_1_scores) / len(rouge_1_scores)\n",
    "    avg_rouge_2 = sum(rouge_2_scores) / len(rouge_2_scores)\n",
    "    avg_rouge_l = sum(rouge_l_scores) / len(rouge_l_scores)\n",
    "    \n",
    "    avg_rouge_1 *= 100\n",
    "    avg_rouge_2 *= 100\n",
    "    avg_rouge_l *= 100\n",
    "    \n",
    "    return avg_rouge_1, avg_rouge_2, avg_rouge_l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METEOR Metric\n",
    "\n",
    "def tokenize(text):\n",
    "    return nltk.word_tokenize(text.lower())\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return synonyms\n",
    "\n",
    "def precision_recall(candidate_tokens, reference_tokens):\n",
    "    candidate_counter = Counter(candidate_tokens)\n",
    "    reference_counter = Counter(reference_tokens)\n",
    "\n",
    "    matches = sum(min(candidate_counter[token], reference_counter[token]) for token in candidate_counter)\n",
    "    precision = matches / len(candidate_tokens) if candidate_tokens else 0.0\n",
    "    \n",
    "    recall = matches / len(reference_tokens) if reference_tokens else 0.0\n",
    "    \n",
    "    return precision, recall\n",
    "\n",
    "def meteor_score(candidate, references):\n",
    "    candidate_tokens = tokenize(candidate)\n",
    "    reference_tokens = [tokenize(ref) for ref in references]\n",
    "    \n",
    "    best_f1 = 0.0\n",
    "    \n",
    "    for ref_tokens in reference_tokens:\n",
    "        precision, recall = precision_recall(candidate_tokens, ref_tokens)\n",
    "        \n",
    "        if precision + recall > 0:\n",
    "            f1_score = (2 * precision * recall) / (precision + recall)\n",
    "            best_f1 = max(best_f1, f1_score)\n",
    "    \n",
    "    return best_f1\n",
    "\n",
    "def calculate_meteor_scores(candidates, references):\n",
    "    assert len(candidates) == len(references), \"Candidates and references must be of the same length\"\n",
    "    \n",
    "    meteor_scores = []\n",
    "    for candidate, reference_list in zip(candidates, references):\n",
    "        meteor_scores.append(meteor_score(candidate, reference_list))\n",
    "    \n",
    "    return sum(meteor_scores) / len(meteor_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLEU Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing BLEU Score of Answer Generation Model 1: 100%|██████████| 100/100 [00:00<00:00, 4083.28it/s]\n",
      "Computing BLEU Score of Answer Generation Model 2: 100%|██████████| 100/100 [00:00<00:00, 12534.53it/s]\n",
      "Computing BLEU Score of Answer Generation Model 3: 100%|██████████| 100/100 [00:00<00:00, 3097.00it/s]\n",
      "Computing BLEU Score of Question Generation Model 1: 100%|██████████| 100/100 [00:00<00:00, 5268.10it/s]\n",
      "Computing BLEU Score of Question Generation Model 2: 100%|██████████| 100/100 [00:00<00:00, 5888.23it/s]\n",
      "Computing BLEU Score of Question Generation Model 3: 100%|██████████| 100/100 [00:00<00:00, 4767.61it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Answer Generation Models\"\"\"\n",
    "\n",
    "total_score_for_agm1 = 0.0\n",
    "\n",
    "for i, (hypothesis, references) in tqdm(enumerate(zip(answer_generation_model1_hyp, bleu_ref_answers)), desc=\"Computing BLEU Score of Answer Generation Model 1\", total=len(df_sample)):\n",
    "    score = compute_bleu_score(references, hypothesis)\n",
    "    total_score_for_agm1 += score\n",
    "\n",
    "percentage_bleu_score_for_agm1  = ( total_score_for_agm1 / len(df_sample) ) * 100\n",
    "\n",
    "total_score_for_agm2 = 0.0\n",
    "\n",
    "for i, (hypothesis, references) in tqdm(enumerate(zip(answer_generation_model2_hyp, bleu_ref_answers)), desc=\"Computing BLEU Score of Answer Generation Model 2\", total=len(df_sample)):\n",
    "    score = compute_bleu_score(references, hypothesis)\n",
    "    total_score_for_agm2 += score\n",
    "\n",
    "percentage_bleu_score_for_agm2  = ( total_score_for_agm2 / len(df_sample) ) * 100\n",
    "\n",
    "\n",
    "total_score_for_agm3 = 0.0\n",
    "\n",
    "for i, (hypothesis, references) in tqdm(enumerate(zip(answer_generation_model3_hyp, bleu_ref_answers)), desc=\"Computing BLEU Score of Answer Generation Model 3\", total=len(df_sample)):\n",
    "    score = compute_bleu_score(references, hypothesis)\n",
    "    total_score_for_agm3 += score\n",
    "\n",
    "percentage_bleu_score_for_agm3  = ( total_score_for_agm3 / len(df_sample) ) * 100\n",
    "\n",
    "\n",
    "\"\"\" =============================================================== \"\"\"\n",
    "\n",
    "\"\"\"Question Generation Models\"\"\"\n",
    "\n",
    "total_score_for_qgm1 = 0.0\n",
    "\n",
    "for i, (hypothesis, references) in tqdm(enumerate(zip(question_generation_model1_hyp, bleu_ref_questions)), desc=\"Computing BLEU Score of Question Generation Model 1\", total=len(df_sample)):\n",
    "    score = compute_bleu_score(references, hypothesis)\n",
    "    total_score_for_qgm1 += score\n",
    "\n",
    "percentage_bleu_score_for_qgm1  = ( total_score_for_qgm1 / len(df_sample) ) * 100\n",
    "\n",
    "\n",
    "total_score_for_qgm2 = 0.0\n",
    "\n",
    "for i, (hypothesis, references) in tqdm(enumerate(zip(question_generation_model2_hyp, bleu_ref_questions)), desc=\"Computing BLEU Score of Question Generation Model 2\", total=len(df_sample)):\n",
    "    score = compute_bleu_score(references, hypothesis)\n",
    "    total_score_for_qgm2 += score\n",
    "\n",
    "percentage_bleu_score_for_qgm2  = ( total_score_for_qgm2 / len(df_sample) ) * 100\n",
    "\n",
    "\n",
    "total_score_for_qgm3 = 0.0\n",
    "\n",
    "for i, (hypothesis, references) in tqdm(enumerate(zip(question_generation_model3_hyp, bleu_ref_questions)), desc=\"Computing BLEU Score of Question Generation Model 3\", total=len(df_sample)):\n",
    "    score = compute_bleu_score(references, hypothesis)\n",
    "    total_score_for_qgm3 += score\n",
    "\n",
    "percentage_bleu_score_for_qgm3  = ( total_score_for_qgm3 / len(df_sample) ) * 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROUGE Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Answer Generation Models\"\"\"\n",
    "\n",
    "avg_rouge_1_for_agm1, avg_rouge_2_for_agm1, avg_rouge_l_for_agm1 = calculate_rouge_scores(answer_generation_model1_hyp, ref_answers)\n",
    "\n",
    "avg_rouge_1_for_agm2, avg_rouge_2_for_agm2, avg_rouge_l_for_agm2 = calculate_rouge_scores(answer_generation_model2_hyp, ref_answers)\n",
    "\n",
    "avg_rouge_1_for_agm3, avg_rouge_2_for_agm3, avg_rouge_l_for_agm3 = calculate_rouge_scores(answer_generation_model3_hyp, ref_answers)\n",
    "\n",
    "\"\"\" =============================================================== \"\"\"\n",
    "\n",
    "\"\"\"Question Generation Models\"\"\"\n",
    "\n",
    "avg_rouge_1_for_qgm1, avg_rouge_2_for_qgm1, avg_rouge_l_for_qgm1 = calculate_rouge_scores(question_generation_model1_hyp, ref_questions)\n",
    "\n",
    "avg_rouge_1_for_qgm2, avg_rouge_2_for_qgm2, avg_rouge_l_for_qgm2 = calculate_rouge_scores(question_generation_model2_hyp, ref_questions)\n",
    "\n",
    "avg_rouge_1_for_qgm3, avg_rouge_2_for_qgm3, avg_rouge_l_for_qgm3 = calculate_rouge_scores(question_generation_model3_hyp, ref_questions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "METEOR Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Answer Generation Models\"\"\"\n",
    "\n",
    "average_meteor_score_for_agm1 = calculate_meteor_scores(answer_generation_model1_hyp, meteor_ref)\n",
    "percentage_meteor_score_for_agm1 = average_meteor_score_for_agm1 * 100\n",
    "\n",
    "average_meteor_score_for_agm2 = calculate_meteor_scores(answer_generation_model2_hyp, meteor_ref)\n",
    "percentage_meteor_score_for_agm2 = average_meteor_score_for_agm2 * 100\n",
    "\n",
    "average_meteor_score_for_agm3 = calculate_meteor_scores(answer_generation_model3_hyp, meteor_ref)\n",
    "percentage_meteor_score_for_agm3 = average_meteor_score_for_agm3 * 100\n",
    "\n",
    "\"\"\" =============================================================== \"\"\"\n",
    "\n",
    "\"\"\"Question Generation Models\"\"\"\n",
    "\n",
    "average_meteor_score_for_qgm1 = calculate_meteor_scores(question_generation_model1_hyp, meteor_ref)\n",
    "percentage_meteor_score_for_qgm1 = average_meteor_score_for_agm1 * 100\n",
    "\n",
    "average_meteor_score_for_qgm2 = calculate_meteor_scores(question_generation_model2_hyp, meteor_ref)\n",
    "percentage_meteor_score_for_qgm2 = average_meteor_score_for_agm2 * 100\n",
    "\n",
    "average_meteor_score_for_qgm3 = calculate_meteor_scores(question_generation_model3_hyp, meteor_ref)\n",
    "percentage_meteor_score_for_qgm3 = average_meteor_score_for_agm3 * 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine Similarity for Sentence Transformers Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_sentence_transformers(ref_contexts)\n",
    "avg_cos_sim1 = average_cosine_similarity(results[\"Model1 (all-MiniLM-L6-v2)\"])\n",
    "avg_cos_sim2 = average_cosine_similarity(results[\"Model2 (msmarco-distilbert-base-v2)\"])\n",
    "avg_cos_sim3 = average_cosine_similarity(results[\"Model3 (LaBSE)\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      \n",
      "=================== BLEU Scores for Answer Generation Models =====================================\n",
      "      \n",
      "      \n",
      "Total BLEU Score for Answer Generation Model 1: 53.17%\")\n",
      "\n",
      "Total BLEU Score for Answer Generation Model 2: 34.19%\")\n",
      "\n",
      "Total BLEU Score for Answer Generation Model 3: 53.44%\")\n",
      "\n",
      "\n",
      "================== ROUGE Scores for Answer Generation Models =====================================\n",
      "\n",
      "\n",
      "Average ROUGE-1 Score of Answer Generation Model 1 : 59.60%\n",
      "Average ROUGE-2 Score of Answer Generation Model 1 : 37.39%\n",
      "Average ROUGE-L Score of Answer Generation Model 1 : 56.79%\n",
      "\n",
      "Average ROUGE-1 Score of Answer Generation Model 2 : 41.32%\n",
      "Average ROUGE-2 Score of Answer Generation Model 2 : 24.68%\n",
      "Average ROUGE-L Score of Answer Generation Model 2 : 40.94%\n",
      "\n",
      "Average ROUGE-1 Score of Answer Generation Model 3 : 37.07%\n",
      "Average ROUGE-2 Score of Answer Generation Model 3 : 18.47%\n",
      "Average ROUGE-L Score of Answer Generation Model 3 : 34.77%\n",
      "\n",
      "\n",
      "================= METEOR Scores for Answer Generation Models =====================================\n",
      "\n",
      "\n",
      "Total METEOR Score for Answer Generation Model 1: 58.68%\")\n",
      "\n",
      "Total METEOR Score for Answer Generation Model 2: 40.77%\")\n",
      "\n",
      "Total METEOR Score for Answer Generation Model 3: 54.57%\")\n",
      "\n",
      "\n",
      "================= BLEU Scores for Question Generation Models =====================================\n",
      "\n",
      "\n",
      "Total BLEU Score for Question Generation Model 1: 42.14%\")\n",
      "\n",
      "Total BLEU Score for Question Generation Model 2: 22.80%\")\n",
      "\n",
      "Total BLEU Score for Question Generation Model 3: 36.64%\")\n",
      "\n",
      "\n",
      "======================== ROUGE Scores for Question Generation Models =============================\n",
      "\n",
      "Average ROUGE-1 Score of Question Generation Model 1 : 39.68%\n",
      "Average ROUGE-2 Score of Question Generation Model 1 : 22.82%\n",
      "Average ROUGE-L Score of Question Generation Model 1 : 38.19%\n",
      "\n",
      "Average ROUGE-1 Score of Question Generation Model 2 : 19.55%\n",
      "Average ROUGE-2 Score of Question Generation Model 2 : 4.20%\n",
      "Average ROUGE-L Score of Question Generation Model 2 : 18.88%\n",
      "\n",
      "Average ROUGE-1 Score of Question Generation Model 3 : 36.55%\n",
      "Average ROUGE-2 Score of Question Generation Model 3 : 18.81%\n",
      "Average ROUGE-L Score of Question Generation Model 3 : 30.76%\n",
      "\n",
      "\n",
      "================= METEOR Scores for Question Generation Models =====================================\n",
      "\n",
      "\n",
      "Total METEOR Score for Question Generation Model 1: 58.68%\")\n",
      "\n",
      "Total METEOR Score for Question Generation Model 2: 40.77%\")\n",
      "\n",
      "Total METEOR Score for Question Generation Model 3: 54.57%\")\n",
      "\n",
      "\n",
      "================================ Sentence Transformer Models =====================================\n",
      "\n",
      "Total Cosine Similarity of Sentence Transformer Model 1 : 2.58%\n",
      "\n",
      "Total Cosine Similarity of Sentence Transformer Model 2 : 4.61%\n",
      "\n",
      "Total Cosine Similarity of Sentence Transformer Model 3 : 32.58%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "      \n",
    "=================== BLEU Scores for Answer Generation Models =====================================\n",
    "      \n",
    "      \n",
    "Total BLEU Score for Answer Generation Model 1: {percentage_bleu_score_for_agm1:.2f}%\")\n",
    "\n",
    "Total BLEU Score for Answer Generation Model 2: {percentage_bleu_score_for_agm2:.2f}%\")\n",
    "\n",
    "Total BLEU Score for Answer Generation Model 3: {percentage_bleu_score_for_agm3:.2f}%\")\n",
    "\n",
    "\n",
    "================== ROUGE Scores for Answer Generation Models =====================================\n",
    "\n",
    "\n",
    "Average ROUGE-1 Score of Answer Generation Model 1 : {avg_rouge_1_for_agm1:.2f}%\n",
    "Average ROUGE-2 Score of Answer Generation Model 1 : {avg_rouge_2_for_agm1:.2f}%\n",
    "Average ROUGE-L Score of Answer Generation Model 1 : {avg_rouge_l_for_agm1:.2f}%\n",
    "\n",
    "Average ROUGE-1 Score of Answer Generation Model 2 : {avg_rouge_1_for_agm2:.2f}%\n",
    "Average ROUGE-2 Score of Answer Generation Model 2 : {avg_rouge_2_for_agm2:.2f}%\n",
    "Average ROUGE-L Score of Answer Generation Model 2 : {avg_rouge_l_for_agm2:.2f}%\n",
    "\n",
    "Average ROUGE-1 Score of Answer Generation Model 3 : {avg_rouge_1_for_agm3:.2f}%\n",
    "Average ROUGE-2 Score of Answer Generation Model 3 : {avg_rouge_2_for_agm3:.2f}%\n",
    "Average ROUGE-L Score of Answer Generation Model 3 : {avg_rouge_l_for_agm3:.2f}%\n",
    "\n",
    "\n",
    "================= METEOR Scores for Answer Generation Models =====================================\n",
    "\n",
    "\n",
    "Total METEOR Score for Answer Generation Model 1: {percentage_meteor_score_for_agm1:.2f}%\")\n",
    "\n",
    "Total METEOR Score for Answer Generation Model 2: {percentage_meteor_score_for_agm2:.2f}%\")\n",
    "\n",
    "Total METEOR Score for Answer Generation Model 3: {percentage_meteor_score_for_agm3:.2f}%\")\n",
    "\n",
    "\n",
    "================= BLEU Scores for Question Generation Models =====================================\n",
    "\n",
    "\n",
    "Total BLEU Score for Question Generation Model 1: {percentage_bleu_score_for_qgm1:.2f}%\")\n",
    "\n",
    "Total BLEU Score for Question Generation Model 2: {percentage_bleu_score_for_qgm2:.2f}%\")\n",
    "\n",
    "Total BLEU Score for Question Generation Model 3: {percentage_bleu_score_for_qgm3:.2f}%\")\n",
    "\n",
    "\n",
    "======================== ROUGE Scores for Question Generation Models =============================\n",
    "\n",
    "Average ROUGE-1 Score of Question Generation Model 1 : {avg_rouge_1_for_qgm1:.2f}%\n",
    "Average ROUGE-2 Score of Question Generation Model 1 : {avg_rouge_2_for_qgm1:.2f}%\n",
    "Average ROUGE-L Score of Question Generation Model 1 : {avg_rouge_l_for_qgm1:.2f}%\n",
    "\n",
    "Average ROUGE-1 Score of Question Generation Model 2 : {avg_rouge_1_for_qgm2:.2f}%\n",
    "Average ROUGE-2 Score of Question Generation Model 2 : {avg_rouge_2_for_qgm2:.2f}%\n",
    "Average ROUGE-L Score of Question Generation Model 2 : {avg_rouge_l_for_qgm2:.2f}%\n",
    "\n",
    "Average ROUGE-1 Score of Question Generation Model 3 : {avg_rouge_1_for_qgm3:.2f}%\n",
    "Average ROUGE-2 Score of Question Generation Model 3 : {avg_rouge_2_for_qgm3:.2f}%\n",
    "Average ROUGE-L Score of Question Generation Model 3 : {avg_rouge_l_for_qgm3:.2f}%\n",
    "\n",
    "\n",
    "================= METEOR Scores for Question Generation Models =====================================\n",
    "\n",
    "\n",
    "Total METEOR Score for Question Generation Model 1: {percentage_meteor_score_for_qgm1:.2f}%\")\n",
    "\n",
    "Total METEOR Score for Question Generation Model 2: {percentage_meteor_score_for_qgm2:.2f}%\")\n",
    "\n",
    "Total METEOR Score for Question Generation Model 3: {percentage_meteor_score_for_qgm3:.2f}%\")\n",
    "\n",
    "\n",
    "================================ Sentence Transformer Models =====================================\n",
    "\n",
    "Total Cosine Similarity of Sentence Transformer Model 1 : {avg_cos_sim1:.2f}%\n",
    "\n",
    "Total Cosine Similarity of Sentence Transformer Model 2 : {avg_cos_sim2:.2f}%\n",
    "\n",
    "Total Cosine Similarity of Sentence Transformer Model 3 : {avg_cos_sim3:.2f}%\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary of Model Selection and Insights\n",
      "\n",
      "Answer Generation Models:\n",
      "----------------------------------------\n",
      "Model 1 has been selected due to its consistent and high performance across multiple metrics:\n",
      "  - BLEU Score: 53.17%, indicating strong alignment with reference texts.\n",
      "  - ROUGE Scores: High ROUGE-1 (59.60%) and ROUGE-L scores (56.79%) suggest good overlap in individual words and longer phrases. However, ROUGE-2 score of 37.39% indicates a lack of bigram overlap.\n",
      "  - METEOR Score: 58.68%, supports effectiveness in capturing meaning and semantic similarity.\n",
      "\n",
      "Comparison with Other Models:\n",
      "  - Model 2 shows lower performance across BLEU (34.19%), ROUGE (for ROUGE-1 (41.32%) and ROUGE-L (40.94%)), and METEOR (0.41%). It seems less effective in generating matching answers and capturing meaning.\n",
      "  - Model 3 has similar BLEU and METEOR scores as Model 1 but lower ROUGE scores, indicating good fluency but less detailed content overlap.\n",
      "\n",
      "Question Generation Models:\n",
      "----------------------------------------\n",
      "Model 1 is chosen for its balanced performance:\n",
      "  - BLEU Score: 42.14%, indicative of its ability to generate coherent questions that align with reference structures.\n",
      "  - ROUGE Scores: ROUGE-1 and ROUGE-L scores (39.68% and 38.19% respectively) show reasonable match with reference questions, though ROUGE-2 (22.82%) suggests room for improvement in capturing bigram matches.\n",
      "  - METEOR Score: 58.68%, reflects good semantic alignment and content coverage.\n",
      "\n",
      "Comparison with Other Models:\n",
      "  - Model 2 has lower BLEU (22.80%) and METEOR scores (40.77%), suggesting it may not capture the intended question structures as effectively.\n",
      "  - Model 3 performs slightly better in BLEU (36.64%) but has lower ROUGE and METEOR scores, indicating higher syntactic similarity but less semantic relevance.\n",
      "\n",
      "Sentence Transformers Models:\n",
      "----------------------------------------\n",
      "Model 3 is preferred due to its highest cosine similarity score (32.58%), indicating the best semantic similarity and coherence among the embeddings.\n",
      "\n",
      "Comparison with Other Models:\n",
      "  - Model 1 and Model 2 have lower cosine similarity scores (2.58% and 4.61%, respectively), indicating they might not capture semantic relationships as effectively as Model 3.\n",
      "\n",
      "Overall Insight:\n",
      "----------------------------------------\n",
      "Choosing Model 1 for answer and question generation will likely result in improved performance due to high BLEU, ROUGE, and METEOR scores, ensuring high-quality and contextually relevant outputs.\n",
      "sentence transformer Model 3 is the top choice for sentence embeddings, providing superior semantic similarity for effective text understanding and comparison.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# summary of model selection and insights\n",
    "\n",
    "print(f\"\"\"\n",
    "Summary of Model Selection and Insights\n",
    "\n",
    "Answer Generation Models:\n",
    "----------------------------------------\n",
    "Model 1 has been selected due to its consistent and high performance across multiple metrics:\n",
    "  - BLEU Score: {percentage_bleu_score_for_agm1:.2f}%, indicating strong alignment with reference texts.\n",
    "  - ROUGE Scores: High ROUGE-1 ({avg_rouge_1_for_agm1:.2f}%) and ROUGE-L scores ({avg_rouge_l_for_agm1:.2f}%) suggest good overlap in individual words and longer phrases. However, ROUGE-2 score of {avg_rouge_2_for_agm1:.2f}% indicates a lack of bigram overlap.\n",
    "  - METEOR Score: {percentage_meteor_score_for_agm1:.2f}%, supports effectiveness in capturing meaning and semantic similarity.\n",
    "\n",
    "Comparison with Other Models:\n",
    "  - Model 2 shows lower performance across BLEU ({percentage_bleu_score_for_agm2:.2f}%), ROUGE (for ROUGE-1 ({avg_rouge_1_for_agm2:.2f}%) and ROUGE-L ({avg_rouge_l_for_agm2:.2f}%)), and METEOR ({average_meteor_score_for_agm2:.2f}%). It seems less effective in generating matching answers and capturing meaning.\n",
    "  - Model 3 has similar BLEU and METEOR scores as Model 1 but lower ROUGE scores, indicating good fluency but less detailed content overlap.\n",
    "\n",
    "Question Generation Models:\n",
    "----------------------------------------\n",
    "Model 1 is chosen for its balanced performance:\n",
    "  - BLEU Score: {percentage_bleu_score_for_qgm1:.2f}%, indicative of its ability to generate coherent questions that align with reference structures.\n",
    "  - ROUGE Scores: ROUGE-1 and ROUGE-L scores ({avg_rouge_1_for_qgm1:.2f}% and {avg_rouge_l_for_qgm1:.2f}% respectively) show reasonable match with reference questions, though ROUGE-2 ({avg_rouge_2_for_qgm1:.2f}%) suggests room for improvement in capturing bigram matches.\n",
    "  - METEOR Score: {percentage_meteor_score_for_qgm1:.2f}%, reflects good semantic alignment and content coverage.\n",
    "\n",
    "Comparison with Other Models:\n",
    "  - Model 2 has lower BLEU ({percentage_bleu_score_for_qgm2:.2f}%) and METEOR scores ({percentage_meteor_score_for_qgm2:.2f}%), suggesting it may not capture the intended question structures as effectively.\n",
    "  - Model 3 performs slightly better in BLEU ({percentage_bleu_score_for_qgm3:.2f}%) but has lower ROUGE and METEOR scores, indicating higher syntactic similarity but less semantic relevance.\n",
    "\n",
    "Sentence Transformers Models:\n",
    "----------------------------------------\n",
    "Model 3 is preferred due to its highest cosine similarity score ({avg_cos_sim3:.2f}%), indicating the best semantic similarity and coherence among the embeddings.\n",
    "\n",
    "Comparison with Other Models:\n",
    "  - Model 1 and Model 2 have lower cosine similarity scores ({avg_cos_sim1:.2f}% and {avg_cos_sim2:.2f}%, respectively), indicating they might not capture semantic relationships as effectively as Model 3.\n",
    "\n",
    "Overall Insight:\n",
    "----------------------------------------\n",
    "Choosing Model 1 for answer and question generation will likely result in improved performance due to high BLEU, ROUGE, and METEOR scores, ensuring high-quality and contextually relevant outputs.\n",
    "sentence transformer Model 3 is the top choice for sentence embeddings, providing superior semantic similarity for effective text understanding and comparison.\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
